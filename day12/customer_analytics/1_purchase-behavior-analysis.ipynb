{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea14ef1-360c-41ec-b787-ebfac825fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68517c44-9c5d-44a9-b4b2-994dae247f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c7a12a1c-1f6f-4615-9922-7efb03b85b72;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 322ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c7a12a1c-1f6f-4615-9922-7efb03b85b72\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/16ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/17 04:01:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"RetailCustomer360\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ab4934-dc82-4ada-ae83-fc56430c1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MySQL table into PySpark DataFrame\n",
    "customer_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "order_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "# Register as Temp Tables\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "order_df.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4b686e-d898-4196-bfa1-2f328682c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+-------------------+\n",
      "|customer_id|first_name|last_name|               email|               phone|             address|               city|         state|zip_code|             country|         created_at|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+-------------------+\n",
      "|          1|   Charles|    Jones|rodneysullivan@ex...| (337)512-3234x99488|365 Ricky Spur\\nK...|         Jacobmouth|        Kansas|   72281|United States of ...|2025-03-17 03:54:54|\n",
      "|          2|   Brandon|  Mcgrath|crystalnavarro@ex...|  953.750.7854x37299|6373 Susan Inlet ...|       Chambersview|      Oklahoma|   20216|          Luxembourg|2025-03-17 03:54:51|\n",
      "|          3|     Misty|   Warren| fpineda@example.com|   315.763.1867x5725|67991 Michael Roa...|         Sarahshire|      Illinois|   37114|         Netherlands|2025-03-17 03:54:47|\n",
      "|          4| Alexander|     Wood|lisajackson@examp...|        552-553-1688|74494 Sarah Grove...|       South Angela|      Delaware|   18701|             Bolivia|2025-03-17 03:54:43|\n",
      "|          5|    Monica|     Ruiz|  dcross@example.org|   378.249.9885x2000|USS Morris\\nFPO A...|          Brownstad| West Virginia|   86237|             Hungary|2025-03-17 03:54:59|\n",
      "|          6|    Nathan|     King|christopherarmstr...|     +1-634-820-8654|45723 Adam Club\\n...|         Juarezberg|        Alaska|   91570|       New Caledonia|2025-03-17 03:54:40|\n",
      "|          7|      Paul|      Lin|michaeljenkins@ex...| (270)846-2155x54374|90485 Barnes View...|        Teresaville|  South Dakota|   41124|               Chile|2025-03-17 03:54:58|\n",
      "|          8|      John|    Jones|nicholas26@exampl...|   931.749.8746x8942|393 Hudson Ridges...|North Veronicaville|  Rhode Island|   03839|           Venezuela|2025-03-17 03:54:49|\n",
      "|          9|    Justin|  Morales| aturner@example.com|          7874652390|8714 Zhang Lake\\n...|   Lake Marychester|     Wisconsin|   75684|           Gibraltar|2025-03-17 03:54:53|\n",
      "|         10|      Lisa|   Howard|tracey41@example.net|       (554)570-9218|292 Garcia Brook\\...|     Port Dustinton|  North Dakota|   09621|             Iceland|2025-03-17 03:54:48|\n",
      "|         11|   Michele|  Herrera|rebecca78@example...|          3729322995|Unit 0737 Box 399...|     West Lynnmouth|      Oklahoma|   82367|           Swaziland|2025-03-17 03:54:52|\n",
      "|         12|     Holly|   Conley|patriciarichardso...|       (754)394-0157|PSC 6888, Box 421...|       East Phyllis|      Maryland|   16667|              Taiwan|2025-03-17 03:54:51|\n",
      "|         13|   Jessica|    Poole|leonard20@example...|          4669207718|9937 Hannah Cliff...|          Emilyview|  North Dakota|   80610|             Namibia|2025-03-17 03:55:01|\n",
      "|         14|     Brian|   Turner| james95@example.org|          4938814142|4924 Emily Pine\\n...|           Timshire|      Maryland|   82261|             Burundi|2025-03-17 03:54:49|\n",
      "|         15|     Chris|    Woods|bakerbrian@exampl...| +1-565-401-4363x947|01667 Jackson Isl...|         Garciafort|South Carolina|   17491|United States Vir...|2025-03-17 03:54:43|\n",
      "|         16|     Ricky|  Wallace|joshuadouglas@exa...|       (532)823-3161|Unit 9674 Box 271...|    West Joshualand|      Kentucky|   62474|United States Min...|2025-03-17 03:54:49|\n",
      "|         17|Alexandria|    Davis|donnamckenzie@exa...|        343-209-9152|802 Rachel Greens...|       North Jeremy|North Carolina|   17004|             Hungary|2025-03-17 03:54:49|\n",
      "|         18|   Matthew|  Andrade|margaretclark@exa...|        452.585.7697|7943 Richard High...|           East Amy|    New Mexico|   68844|              Taiwan|2025-03-17 03:54:59|\n",
      "|         19|  Kathleen|     Sims|belindawilson@exa...|001-962-729-4641x...|816 Lee Route Apt...|      West Johnland|      Delaware|   85760|              Uganda|2025-03-17 03:54:39|\n",
      "|         20|      Dale| Harrison|gloriaholloway@ex...|     +1-612-637-6483|378 Hernandez Mil...|      South Russell|  Rhode Island|   25221|        Saudi Arabia|2025-03-17 03:54:54|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133c907a-d105-4925-a326-84ea17dee80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+---------+------------+\n",
      "|order_id|customer_id|         order_date|   status|total_amount|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "|    2256|        553|2025-01-05 00:00:00|Cancelled|      392.96|\n",
      "|   12065|        553|2020-06-01 00:00:00|  Pending|      275.97|\n",
      "|   12586|        553|2023-08-31 00:00:00|  Shipped|       87.41|\n",
      "|   19939|        553|2022-07-25 00:00:00|Cancelled|       28.39|\n",
      "|   20799|        553|2022-07-18 00:00:00|  Pending|      388.93|\n",
      "|   24883|        553|2020-05-02 00:00:00|Delivered|      289.27|\n",
      "|   29740|        553|2023-09-15 00:00:00|  Pending|      104.74|\n",
      "|   31874|        553|2023-02-09 00:00:00|  Pending|      229.42|\n",
      "|   41874|        553|2020-04-12 00:00:00|  Pending|      161.43|\n",
      "|   56057|        553|2023-09-11 00:00:00|  Shipped|      296.85|\n",
      "|   57301|        553|2021-06-27 00:00:00|Delivered|       80.25|\n",
      "|   59242|        553|2024-11-16 00:00:00|Delivered|      441.85|\n",
      "|   61722|        553|2022-01-05 00:00:00|  Shipped|      201.45|\n",
      "|   67419|        553|2022-03-30 00:00:00|Delivered|      127.35|\n",
      "|   75608|        553|2022-01-24 00:00:00|Delivered|       88.13|\n",
      "|   93843|        553|2024-01-23 00:00:00|  Shipped|      232.10|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *   FROM orders o  where o.customer_id=553 \"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1c3429-acc6-4d94-adc9-7cae792f5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|       2333|    7776.53|\n",
      "|       9408|    6713.81|\n",
      "|        949|    6480.47|\n",
      "|       2171|    6462.36|\n",
      "|       6018|    6254.54|\n",
      "|       6864|    6232.70|\n",
      "|       5758|    6159.81|\n",
      "|       7130|    6135.97|\n",
      "|       1643|    6131.51|\n",
      "|       6056|    6064.61|\n",
      "|       1439|    6013.41|\n",
      "|       4027|    5953.78|\n",
      "|       3294|    5952.49|\n",
      "|       5210|    5848.03|\n",
      "|       1494|    5830.34|\n",
      "|       2350|    5820.18|\n",
      "|        402|    5814.20|\n",
      "|       3960|    5780.35|\n",
      "|       2601|    5751.75|\n",
      "|        545|    5742.82|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent\n",
    "        FROM orders o GROUP BY o.customer_id  order by total_spent desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190428bf-3f49-49c1-a603-baf117748d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-------------------+\n",
      "|customer_id|total_spent|total_orders| last_purchase_date|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "|       1531|    2969.88|           8|2025-02-14 00:00:00|\n",
      "|       5386|    2794.61|           8|2025-03-13 00:00:00|\n",
      "|         47|    2554.18|           7|2024-11-09 00:00:00|\n",
      "|       6957|    2497.63|           7|2025-03-03 00:00:00|\n",
      "|       4144|    2429.91|           6|2025-01-19 00:00:00|\n",
      "|       2601|    2339.36|           8|2025-03-09 00:00:00|\n",
      "|       9258|    2293.19|           7|2025-03-01 00:00:00|\n",
      "|       9405|    2277.66|           7|2024-12-24 00:00:00|\n",
      "|       1456|    2267.00|           7|2025-01-13 00:00:00|\n",
      "|       7315|    2263.81|           6|2025-01-27 00:00:00|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_customers = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o \n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc  \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570d5292-2fb0-4944-8b31-b8949fec3638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|       country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Kingdom|       7895|   Kenneth|  pestes@example.org|    1693.16|           5|2024-11-24 00:00:00|            1|\n",
      "|United Kingdom|        545|  Lawrence|chopkins@example.org|    1490.65|           5|2025-03-09 00:00:00|            2|\n",
      "|United Kingdom|       3265|   Shannon|kathrynmccarthy@e...|    1404.09|           6|2025-02-24 00:00:00|            3|\n",
      "|United Kingdom|       5438|    Edward|dorothywalker@exa...|    1368.42|           6|2025-01-17 00:00:00|            4|\n",
      "|United Kingdom|       9793|   Anthony|justin90@example.com|    1283.33|           4|2025-02-22 00:00:00|            5|\n",
      "|United Kingdom|        591|    Nicole|sharpjuan@example...|    1183.76|           3|2024-12-20 00:00:00|            6|\n",
      "|United Kingdom|       4393|    Sheila|   ylutz@example.net|     929.08|           2|2024-12-24 00:00:00|            7|\n",
      "|United Kingdom|        882|      John|contreraserika@ex...|     856.39|           2|2024-09-21 00:00:00|            8|\n",
      "|United Kingdom|       4257| Christina|danielmoore@examp...|     842.44|           2|2024-07-24 00:00:00|            9|\n",
      "|United Kingdom|       6072|   Lindsay|darlenemartin@exa...|     825.27|           4|2025-02-26 00:00:00|           10|\n",
      "|United Kingdom|       4023|     Stacy|waltersheather@ex...|     810.41|           4|2024-12-15 00:00:00|           11|\n",
      "|United Kingdom|       8531|   Heather|lauraerickson@exa...|     796.70|           3|2024-09-04 00:00:00|           12|\n",
      "|United Kingdom|       6882|   Melanie|johnrivera@exampl...|     790.00|           3|2024-12-27 00:00:00|           13|\n",
      "|United Kingdom|       4138|  Virginia|gregorybrown@exam...|     785.57|           4|2025-02-25 00:00:00|           14|\n",
      "|United Kingdom|       2639|     Sarah|taylor43@example.com|     759.97|           3|2024-12-06 00:00:00|           15|\n",
      "|United Kingdom|        900|    Andrew|nicole19@example.org|     715.23|           3|2024-11-19 00:00:00|           16|\n",
      "|United Kingdom|       4640|      Cody| aaron90@example.com|     679.90|           2|2024-08-15 00:00:00|           17|\n",
      "|United Kingdom|       5567|   Charles| blake58@example.net|     642.83|           2|2025-01-21 00:00:00|           18|\n",
      "|United Kingdom|       2834|    Joshua|patrickhowe@examp...|     615.25|           3|2024-10-08 00:00:00|           19|\n",
      "|United Kingdom|       5768|      Sean|lawrence99@exampl...|     587.15|           2|2024-12-02 00:00:00|           20|\n",
      "|United Kingdom|       2202|    Brandi|vaughnsheila@exam...|     583.06|           2|2025-02-17 00:00:00|           21|\n",
      "|United Kingdom|       3585| Stephanie|   blynn@example.com|     539.29|           2|2024-05-25 00:00:00|           22|\n",
      "|United Kingdom|       5361|     Laura|cristianowens@exa...|     480.10|           2|2024-12-28 00:00:00|           23|\n",
      "|United Kingdom|       2289|     Jesus|psanchez@example.com|     461.94|           2|2025-01-03 00:00:00|           24|\n",
      "|United Kingdom|       7258|     Gregg|carterkeith@examp...|     417.31|           2|2024-10-13 00:00:00|           25|\n",
      "|United Kingdom|       2629| Elizabeth|  tsmith@example.com|     411.75|           1|2024-06-27 00:00:00|           26|\n",
      "|United Kingdom|       2783|   Timothy|markhale@example.org|     395.29|           1|2024-12-30 00:00:00|           27|\n",
      "|United Kingdom|       1863|      Juan|williamlamb@examp...|     286.39|           1|2024-12-31 00:00:00|           28|\n",
      "|United Kingdom|       2929|   Natalie|christopher07@exa...|     258.03|           2|2025-02-09 00:00:00|           29|\n",
      "|United Kingdom|       7225| Christine|     rle@example.com|     250.06|           1|2024-11-11 00:00:00|           30|\n",
      "|United Kingdom|       8711|     Jacob|  ehogan@example.com|     195.91|           1|2025-02-17 00:00:00|           31|\n",
      "|United Kingdom|         48|  Kristina|millerkristie@exa...|     117.46|           2|2024-08-15 00:00:00|           32|\n",
      "|United Kingdom|       7903|     Derek|wellsnicholas@exa...|      93.42|           2|2025-01-12 00:00:00|           33|\n",
      "|United Kingdom|       6479|     Sarah|  ppoole@example.org|      49.59|           1|2024-05-14 00:00:00|           34|\n",
      "|United Kingdom|       9036|     Scott|murphycharles@exa...|      45.69|           1|2024-07-05 00:00:00|           35|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)   -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc \n",
    "    )\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER ( PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id where c.country='United Kingdom'  GROUP BY c.country,c.customer_id,c.first_name,c.email,cs.total_spent,cs.last_purchase_date,cs.total_orders order by total_spent desc\n",
    "\"\"\").show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958dee8b-983b-4384-86e0-befda5cbddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|             country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Arab Emirates|       3222|    George|jacobpeterson@exa...|    1487.63|           4|2024-10-06 00:00:00|            1|\n",
      "|United Arab Emirates|       9505|     Tonya|medinalisa@exampl...|    1149.79|           4|2025-02-03 00:00:00|            2|\n",
      "|United Arab Emirates|        752|   William|josebradford@exam...|    1111.42|           3|2024-08-24 00:00:00|            3|\n",
      "|United Arab Emirates|       3814|   Krystal|jbradley@example.net|    1080.19|           3|2025-02-24 00:00:00|            4|\n",
      "|United Arab Emirates|       6528|   Tabitha|  rlewis@example.net|     974.51|           4|2025-03-06 00:00:00|            5|\n",
      "|United Arab Emirates|       7612|     Roger| lnelson@example.net|     954.26|           4|2025-01-31 00:00:00|            6|\n",
      "|United Arab Emirates|       9402|    Lauren| thunter@example.org|     790.43|           2|2025-01-26 00:00:00|            7|\n",
      "|United Arab Emirates|       3084|     Wyatt|josesavage@exampl...|     763.10|           2|2025-02-23 00:00:00|            8|\n",
      "|United Arab Emirates|       6526|    Steven|   cchoi@example.com|     737.88|           2|2025-01-03 00:00:00|            9|\n",
      "|United Arab Emirates|         29|     Tyler|   bnash@example.org|     692.72|           2|2025-01-21 00:00:00|           10|\n",
      "|      United Kingdom|       7895|   Kenneth|  pestes@example.org|    1693.16|           5|2024-11-24 00:00:00|            1|\n",
      "|      United Kingdom|        545|  Lawrence|chopkins@example.org|    1490.65|           5|2025-03-09 00:00:00|            2|\n",
      "|      United Kingdom|       3265|   Shannon|kathrynmccarthy@e...|    1404.09|           6|2025-02-24 00:00:00|            3|\n",
      "|      United Kingdom|       5438|    Edward|dorothywalker@exa...|    1368.42|           6|2025-01-17 00:00:00|            4|\n",
      "|      United Kingdom|       9793|   Anthony|justin90@example.com|    1283.33|           4|2025-02-22 00:00:00|            5|\n",
      "|      United Kingdom|        591|    Nicole|sharpjuan@example...|    1183.76|           3|2024-12-20 00:00:00|            6|\n",
      "|      United Kingdom|       4393|    Sheila|   ylutz@example.net|     929.08|           2|2024-12-24 00:00:00|            7|\n",
      "|      United Kingdom|        882|      John|contreraserika@ex...|     856.39|           2|2024-09-21 00:00:00|            8|\n",
      "|      United Kingdom|       4257| Christina|danielmoore@examp...|     842.44|           2|2024-07-24 00:00:00|            9|\n",
      "|      United Kingdom|       6072|   Lindsay|darlenemartin@exa...|     825.27|           4|2025-02-26 00:00:00|           10|\n",
      "|United States Min...|       7124|      Lisa|harrisonjulie@exa...|    1173.16|           4|2025-02-21 00:00:00|            1|\n",
      "|United States Min...|       1210|   Jasmine|  sean55@example.org|    1125.94|           4|2025-02-22 00:00:00|            2|\n",
      "|United States Min...|       9230|    Amanda|brittany50@exampl...|    1065.61|           3|2024-08-26 00:00:00|            3|\n",
      "|United States Min...|       5085|     Brian|  xcline@example.net|    1038.36|           3|2024-09-13 00:00:00|            4|\n",
      "|United States Min...|        125|   Katelyn|   rbell@example.org|     962.34|           4|2025-03-14 00:00:00|            5|\n",
      "|United States Min...|       5852|   Michael|whitney75@example...|     958.52|           4|2025-03-02 00:00:00|            6|\n",
      "|United States Min...|       8396|      Mark|armstrongmary@exa...|     889.17|           3|2025-02-11 00:00:00|            7|\n",
      "|United States Min...|       8420|   Natalie|  rita04@example.org|     673.13|           2|2024-10-18 00:00:00|            8|\n",
      "|United States Min...|       8373|     David|floresjacob@examp...|     662.75|           3|2024-10-23 00:00:00|            9|\n",
      "|United States Min...|       4327|   Suzanne| logan21@example.net|     638.09|           2|2024-09-28 00:00:00|           10|\n",
      "|United States Vir...|       5880|     Jared|welchchristine@ex...|    1753.16|           7|2024-11-17 00:00:00|            1|\n",
      "|United States Vir...|       9946|    Justin|hobbsrebecca@exam...|    1324.22|           5|2024-12-13 00:00:00|            2|\n",
      "|United States Vir...|       7230|     Bryan|  nterry@example.net|    1303.23|           4|2025-03-02 00:00:00|            3|\n",
      "|United States Vir...|       3523|   Spencer| xrogers@example.net|    1215.42|           5|2025-02-27 00:00:00|            4|\n",
      "|United States Vir...|       1546|     Scott|johnhodge@example...|    1035.47|           3|2025-03-01 00:00:00|            5|\n",
      "|United States Vir...|       1110|    Robert|justin70@example.net|     974.27|           3|2024-08-30 00:00:00|            6|\n",
      "|United States Vir...|       6053|     Emily|  fweeks@example.com|     930.31|           2|2024-10-05 00:00:00|            7|\n",
      "|United States Vir...|       7994|   Melanie|  jwoods@example.com|     918.10|           4|2025-02-23 00:00:00|            8|\n",
      "|United States Vir...|       1023|     David|michael10@example...|     917.27|           3|2025-01-22 00:00:00|            9|\n",
      "|United States Vir...|       1513|    Miguel| ltucker@example.net|     827.33|           3|2025-03-03 00:00:00|           10|\n",
      "|United States of ...|       8902|    Angela|kylescott@example...|    1709.44|           6|2024-12-13 00:00:00|            1|\n",
      "|United States of ...|       7066|     Erika|williamsfrancis@e...|    1628.44|           4|2025-02-15 00:00:00|            2|\n",
      "|United States of ...|       6076|    Brenda|pacebrooke@exampl...|    1459.82|           4|2025-02-20 00:00:00|            3|\n",
      "|United States of ...|       7229|   Jessica| bmiller@example.com|    1126.06|           5|2025-02-18 00:00:00|            4|\n",
      "|United States of ...|       4025|   Charles| ybooker@example.net|     859.95|           3|2025-01-16 00:00:00|            5|\n",
      "|United States of ...|       3476|       Amy|daniel91@example.org|     828.05|           2|2024-11-05 00:00:00|            6|\n",
      "|United States of ...|       5486|     Tammy|benjamin96@exampl...|     814.51|           3|2025-01-12 00:00:00|            7|\n",
      "|United States of ...|       9914|     Ethan| mweaver@example.org|     806.67|           2|2024-05-21 00:00:00|            8|\n",
      "|United States of ...|       7305|      John|manuelflores@exam...|     799.08|           3|2024-09-05 00:00:00|            9|\n",
      "|United States of ...|       9041|    Joshua|qhernandez@exampl...|     757.84|           3|2025-01-02 00:00:00|           10|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Spark SQL Query\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id\n",
    "    ),\n",
    "    customer_ranking AS (\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER (PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id\n",
    "    )\n",
    "    SELECT * FROM customer_ranking WHERE spending_rank <= 10 and country like 'United %';\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "top_customers.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94b9c2e-8f02-4d45-beb5-94528c3ecbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------+\n",
      "|             country|customer_id|first_name|               email|total_spent|total_orders|spending_rank|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------+\n",
      "|United Arab Emirates|       3222|    George|jacobpeterson@exa...|    1487.63|           4|            1|\n",
      "|United Arab Emirates|       9505|     Tonya|medinalisa@exampl...|    1149.79|           4|            2|\n",
      "|United Arab Emirates|        752|   William|josebradford@exam...|    1111.42|           3|            3|\n",
      "|United Arab Emirates|       3814|   Krystal|jbradley@example.net|    1080.19|           3|            4|\n",
      "|United Arab Emirates|       6528|   Tabitha|  rlewis@example.net|     974.51|           4|            5|\n",
      "|United Arab Emirates|       7612|     Roger| lnelson@example.net|     954.26|           4|            6|\n",
      "|United Arab Emirates|       9402|    Lauren| thunter@example.org|     790.43|           2|            7|\n",
      "|United Arab Emirates|       3084|     Wyatt|josesavage@exampl...|     763.10|           2|            8|\n",
      "|United Arab Emirates|       6526|    Steven|   cchoi@example.com|     737.88|           2|            9|\n",
      "|United Arab Emirates|         29|     Tyler|   bnash@example.org|     692.72|           2|           10|\n",
      "|      United Kingdom|       7895|   Kenneth|  pestes@example.org|    1693.16|           5|            1|\n",
      "|      United Kingdom|        545|  Lawrence|chopkins@example.org|    1490.65|           5|            2|\n",
      "|      United Kingdom|       3265|   Shannon|kathrynmccarthy@e...|    1404.09|           6|            3|\n",
      "|      United Kingdom|       5438|    Edward|dorothywalker@exa...|    1368.42|           6|            4|\n",
      "|      United Kingdom|       9793|   Anthony|justin90@example.com|    1283.33|           4|            5|\n",
      "|      United Kingdom|        591|    Nicole|sharpjuan@example...|    1183.76|           3|            6|\n",
      "|      United Kingdom|       4393|    Sheila|   ylutz@example.net|     929.08|           2|            7|\n",
      "|      United Kingdom|        882|      John|contreraserika@ex...|     856.39|           2|            8|\n",
      "|      United Kingdom|       4257| Christina|danielmoore@examp...|     842.44|           2|            9|\n",
      "|      United Kingdom|       6072|   Lindsay|darlenemartin@exa...|     825.27|           4|           10|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Filter last 1 year of data\n",
    "one_year_ago = F.date_add(F.current_date(), -365)\n",
    "filtered_orders = order_df.filter(F.col(\"order_date\") >= one_year_ago)\n",
    "\n",
    "# Aggregate customer spending\n",
    "customer_spending = (\n",
    "    filtered_orders.groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.max(\"order_date\").alias(\"last_purchase_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with customers table\n",
    "customer_data = customer_spending.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Define window specification for ranking\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.desc(\"total_spent\"))\n",
    "\n",
    "# Add ranking column\n",
    "customer_ranking = customer_data.withColumn(\"spending_rank\", F.rank().over(window_spec))\n",
    "\n",
    "# Filter top 100 customers\n",
    "top_customers = customer_ranking.filter( (F.col(\"spending_rank\") <= 10) &  (F.col(\"country\").like(\"United %\")))\n",
    "\n",
    "# Show results\n",
    "top_customers.select(\"country\",\"customer_id\",\"first_name\",\"email\",\"total_spent\",\"total_orders\",\"spending_rank\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abae0a8-6c0d-4787-87bc-bffa5522979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/17 04:01:47 INFO HiveConf: Found configuration file file:/home/glue_user/spark/conf/hive-site.xml\n",
      "25/03/17 04:01:49 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/17 04:01:49 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/17 04:01:49 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/17 04:01:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/17 04:02:00 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d0d075f4-c132-4579-b131-e13101572bb8, clientType=HIVECLI]\n",
      "25/03/17 04:02:00 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/03/17 04:02:00 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "25/03/17 04:02:00 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/17 04:02:00 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/17 04:02:00 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/17 04:02:00 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/17 04:02:00 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/17 04:02:00 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sales data written to S3: s3://feb2025-training-bucket/analytics/top_100_customers/\n",
      "Glue table 'customer_analytics.top_100_customers' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define AWS Glue database and table names\n",
    "glue_database = \"customer_analytics\"\n",
    "glue_table = \"top_100_customers\"\n",
    "\n",
    "# Define S3 output path\n",
    "s3_output_path = \"s3://feb2025-training-bucket/analytics/top_100_customers/\"\n",
    "\n",
    "# Create the AWS Glue Catalog table using the DataFrame\n",
    "top_customers.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", s3_output_path) \\\n",
    "    .saveAsTable(f\"{glue_database}.{glue_table}\")\n",
    "\n",
    "print(f\"Aggregated sales data written to S3: {s3_output_path}\")\n",
    "print(f\"Glue table '{glue_database}.{glue_table}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd734bb2-b506-49cc-a469-f69517a718a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
