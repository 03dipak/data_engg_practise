{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751bf541-3371-4469-b3a1-a76449c055f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1d0e0f-4fda-412e-a6e8-1711b93fb1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faker in /home/glue_user/.local/lib/python3.10/site-packages (37.0.0)\n",
      "Requirement already satisfied: tzdata in /home/glue_user/.local/lib/python3.10/site-packages (from faker) (2025.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9258e1-b5d2-4526-8b8a-4b214bf4e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cb0e1003-315b-4ec3-baf7-1f94c7e6367e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 182ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cb0e1003-315b-4ec3-baf7-1f94c7e6367e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/16ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"load_operation_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403fbdd2-43bb-41cb-bfe3-7d836723017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker for realistic data\n",
    "fake = Faker()\n",
    "\n",
    "# Generate Customers\n",
    "def generate_customers(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.first_name(), fake.last_name(), fake.email(), fake.phone_number(),\n",
    "          fake.address(), fake.city(), fake.state(), fake.zipcode(), fake.country()) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"zip_code\", \"country\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c893ab-171b-49e9-8825-fb2abfec9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+\n",
      "|customer_id|first_name|last_name|               email|               phone|             address|               city|         state|zip_code|             country|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+\n",
      "|          1|   Charles|    Jones|rodneysullivan@ex...| (337)512-3234x99488|365 Ricky Spur\\nK...|         Jacobmouth|        Kansas|   72281|United States of ...|\n",
      "|          2|   Brandon|  Mcgrath|crystalnavarro@ex...|  953.750.7854x37299|6373 Susan Inlet ...|       Chambersview|      Oklahoma|   20216|          Luxembourg|\n",
      "|          3|     Misty|   Warren| fpineda@example.com|   315.763.1867x5725|67991 Michael Roa...|         Sarahshire|      Illinois|   37114|         Netherlands|\n",
      "|          4| Alexander|     Wood|lisajackson@examp...|        552-553-1688|74494 Sarah Grove...|       South Angela|      Delaware|   18701|             Bolivia|\n",
      "|          5|    Monica|     Ruiz|  dcross@example.org|   378.249.9885x2000|USS Morris\\nFPO A...|          Brownstad| West Virginia|   86237|             Hungary|\n",
      "|          6|    Nathan|     King|christopherarmstr...|     +1-634-820-8654|45723 Adam Club\\n...|         Juarezberg|        Alaska|   91570|       New Caledonia|\n",
      "|          7|      Paul|      Lin|michaeljenkins@ex...| (270)846-2155x54374|90485 Barnes View...|        Teresaville|  South Dakota|   41124|               Chile|\n",
      "|          8|      John|    Jones|nicholas26@exampl...|   931.749.8746x8942|393 Hudson Ridges...|North Veronicaville|  Rhode Island|   03839|           Venezuela|\n",
      "|          9|    Justin|  Morales| aturner@example.com|          7874652390|8714 Zhang Lake\\n...|   Lake Marychester|     Wisconsin|   75684|           Gibraltar|\n",
      "|         10|      Lisa|   Howard|tracey41@example.net|       (554)570-9218|292 Garcia Brook\\...|     Port Dustinton|  North Dakota|   09621|             Iceland|\n",
      "|         11|   Michele|  Herrera|rebecca78@example...|          3729322995|Unit 0737 Box 399...|     West Lynnmouth|      Oklahoma|   82367|           Swaziland|\n",
      "|         12|     Holly|   Conley|patriciarichardso...|       (754)394-0157|PSC 6888, Box 421...|       East Phyllis|      Maryland|   16667|              Taiwan|\n",
      "|         13|   Jessica|    Poole|leonard20@example...|          4669207718|9937 Hannah Cliff...|          Emilyview|  North Dakota|   80610|             Namibia|\n",
      "|         14|     Brian|   Turner| james95@example.org|          4938814142|4924 Emily Pine\\n...|           Timshire|      Maryland|   82261|             Burundi|\n",
      "|         15|     Chris|    Woods|bakerbrian@exampl...| +1-565-401-4363x947|01667 Jackson Isl...|         Garciafort|South Carolina|   17491|United States Vir...|\n",
      "|         16|     Ricky|  Wallace|joshuadouglas@exa...|       (532)823-3161|Unit 9674 Box 271...|    West Joshualand|      Kentucky|   62474|United States Min...|\n",
      "|         17|Alexandria|    Davis|donnamckenzie@exa...|        343-209-9152|802 Rachel Greens...|       North Jeremy|North Carolina|   17004|             Hungary|\n",
      "|         18|   Matthew|  Andrade|margaretclark@exa...|        452.585.7697|7943 Richard High...|           East Amy|    New Mexico|   68844|              Taiwan|\n",
      "|         19|  Kathleen|     Sims|belindawilson@exa...|001-962-729-4641x...|816 Lee Route Apt...|      West Johnland|      Delaware|   85760|              Uganda|\n",
      "|         20|      Dale| Harrison|gloriaholloway@ex...|     +1-612-637-6483|378 Hernandez Mil...|      South Russell|  Rhode Island|   25221|        Saudi Arabia|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-------------------+--------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate Data\n",
    "customers_df = generate_customers(10000)\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a684edff-d8d8-4e71-a3b2-611510cb17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to MySQL!\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate emails before writing to MySQL\n",
    "customers_df = customers_df.dropDuplicates([\"email\"])\n",
    "\n",
    "customers_df.write.jdbc(url=USER_MYSQL_URL, table=\"Customers\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315fae67-1777-4f90-91bd-00e7c87f9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Products\n",
    "def generate_products(n):\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Home & Kitchen\", \"Books\", \"Toys\", \"Sports\"]\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.word().capitalize(), fake.sentence(nb_words=6), \n",
    "          random.choice(categories), round(random.uniform(5, 500), 2), random.randint(10, 1000)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"product_id\", \"name\", \"description\", \"category\", \"price\", \"stock_quantity\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Payments\n",
    "def generate_payments(n, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_order_id), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n, max_customer_id, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n, max_customer_id, max_product_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_product_id), \n",
    "          fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n, max_customer_id, max_order_id, max_payment_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          random.randint(1, max_payment_id), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678465d-efb9-4285-9fc4-1df92b496282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1e42e6-fc97-4c45-b6be-d697c505126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = generate_products(1000)\n",
    "\n",
    "products_df.write.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf86dde5-8a0b-4b1b-bc7e-a0487b7f70b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+---------+------------+\n",
      "|order_id|customer_id|order_date|   status|total_amount|\n",
      "+--------+-----------+----------+---------+------------+\n",
      "|       1|       3856|2024-06-25|  Pending|      100.13|\n",
      "|       2|       3671|2020-08-07|Delivered|      101.84|\n",
      "|       3|       5160|2024-08-05|Delivered|      476.69|\n",
      "|       4|       6403|2020-06-08|Cancelled|      221.15|\n",
      "|       5|       3469|2021-10-22|Cancelled|      390.76|\n",
      "|       6|       2636|2022-04-08|  Pending|      214.12|\n",
      "|       7|       3706|2023-01-15|Delivered|       69.41|\n",
      "|       8|       6482|2021-01-26|Delivered|      452.21|\n",
      "|       9|       3540|2024-04-21|  Pending|      222.33|\n",
      "|      10|       2356|2024-01-01|Cancelled|      244.17|\n",
      "|      11|        630|2023-11-14|Delivered|       308.8|\n",
      "|      12|       2607|2023-04-21|  Pending|       37.98|\n",
      "|      13|       5700|2021-06-27|Delivered|      140.21|\n",
      "|      14|        494|2022-05-26|Cancelled|      325.17|\n",
      "|      15|       9003|2021-11-06|Delivered|      210.04|\n",
      "|      16|       2486|2025-03-14|  Shipped|       28.08|\n",
      "|      17|       5590|2023-06-17|  Shipped|      447.89|\n",
      "|      18|       5693|2021-01-30|  Pending|       440.5|\n",
      "|      19|       3681|2025-01-01|  Pending|      420.85|\n",
      "|      20|       5154|2025-01-01|  Shipped|       24.42|\n",
      "+--------+-----------+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customers_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "\n",
    "# ✅ Generate Orders using only valid customer IDs\n",
    "def generate_orders(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Pending\", \"Shipped\", \"Delivered\", \"Cancelled\"]), \n",
    "          round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_id\", \"customer_id\", \"order_date\", \"status\", \"total_amount\"]\n",
    "    )\n",
    "\n",
    "orders_df = generate_orders(100000)\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fddd3d9-b125-46b3-91f7-084c438f86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "orders_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166d8144-9234-4f07-8872-1af7e64d46b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#orders_df.write.jdbc(url=MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "orders_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "valid_order_ids = [row[\"order_id\"] for row in orders_df.select(\"order_id\").collect()]\n",
    "valid_product_ids = [row[\"product_id\"] for row in products_df.select(\"product_id\").collect()]\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "# ✅ Generate Order Items using only valid order_id & product_id\n",
    "def generate_order_items(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_order_ids), random.choice(valid_product_ids), \n",
    "          random.randint(1, 5), round(random.uniform(5, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_item_id\", \"order_id\", \"product_id\", \"quantity\", \"unit_price\"]\n",
    "    )\n",
    "order_items_df = generate_order_items(100000)\n",
    "order_items_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Order_Items\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed681e4f-a7db-4b96-aed9-0d950de2e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate Payments\n",
    "def generate_payments(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_order_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "payments_df = generate_payments(10000)\n",
    "valid_payments_ids = [row[\"payment_id\"] for row in payments_df.select(\"payment_id\").collect()]\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "payments_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Payments\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "purchase_history_df = generate_purchase_history(1000)\n",
    "purchase_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Purchase_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb8499c8-bf7a-4b00-bbf6-a38667e40133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to MySQL!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids),random.choice(valid_product_ids),fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n,):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          random.choice(valid_payments_ids), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "usage_history_df = generate_usage_history(1000)\n",
    "usage_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Usage_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "transactions_df = generate_transactions(1000)\n",
    "transactions_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Transactions\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069b0ab-89a1-4102-9c60-4c639a9b8dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,rand,lit\n",
    "# Generate random login data\n",
    "def generate_logins(customers_df, num_attempts=50):\n",
    "    logins = []\n",
    "    for row in customers_df.collect():\n",
    "        customer_id = row[\"customer_id\"]\n",
    "        num_logins = random.randint(1, num_attempts)  # Random login attempts per customer\n",
    "        \n",
    "        for _ in range(num_logins):\n",
    "            ip = f\"192.168.{random.randint(0, 255)}.{random.randint(0, 255)}\"\n",
    "            login_attempt = random.choice([\"SUCCESS\", \"FAILURE\"])\n",
    "            login_date = f\"2024-02-{random.randint(1, 28)}\"\n",
    "            \n",
    "            logins.append((customer_id, ip, login_attempt, login_date))\n",
    "    \n",
    "    return logins\n",
    "\n",
    "# Generate synthetic login data\n",
    "login_data = generate_logins(customers_df)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"customer_id\", \"ip_address\", \"login_attempt\", \"login_date\"]\n",
    "login_df = spark.createDataFrame(login_data, columns)\n",
    "\n",
    "# Introduce suspicious activity (customers with >3 IPs or >10 attempts)\n",
    "login_df = login_df.withColumn(\"is_suspicious\", when((rand() < 0.2), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Write to MySQL (optional)\n",
    "login_df.write.jdbc(url=USER_MYSQL_URL, table=\"LoginHistory\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "# Show result\n",
    "login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2696a-9227-43f4-939b-b782bf27dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
