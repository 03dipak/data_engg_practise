{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe6bcd6-6952-4341-8563-f7a1f040356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ee6e14-80cd-4f46-a73d-99c039068b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-798d30f4-9cc7-43a5-a036-62fd30d8d525;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 276ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-798d30f4-9cc7-43a5-a036-62fd30d8d525\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/17ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/03 13:05:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/03 13:05:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, avg, sum, count\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PricingTrendAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7021d0-bee8-4ca2-917c-18ee252fe0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+-------+------------+\n",
      "|order_id|customer_id|         order_date| status|total_amount|\n",
      "+--------+-----------+-------------------+-------+------------+\n",
      "|       1|       9062|2024-11-28 00:00:00|Shipped|       85.36|\n",
      "|       2|       1918|2021-03-08 00:00:00|Pending|      115.12|\n",
      "+--------+-----------+-------------------+-------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+--------+----------+--------+----------+-----------+\n",
      "|order_item_id|order_id|product_id|quantity|unit_price|total_price|\n",
      "+-------------+--------+----------+--------+----------+-----------+\n",
      "|            1|    5932|        20|       3|     91.54|       null|\n",
      "|            2|    3128|       784|       5|    205.25|       null|\n",
      "+-------------+--------+----------+--------+----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read MySQL table into PySpark DataFrame\n",
    "customer_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "order_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "order_items_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Order_Items\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "# Register as Temp Tables\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "order_df.createOrReplaceTempView(\"orders\")\n",
    "order_items_df.createOrReplaceTempView(\"order_items\")\n",
    "order_df.show(2)\n",
    "order_items_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ebe8e4-e84b-43c2-beb2-6522ca7bd5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+----------+--------+----------+------------+\n",
      "|order_id|customer_id|order_date         |product_id|quantity|unit_price|sales_amount|\n",
      "+--------+-----------+-------------------+----------+--------+----------+------------+\n",
      "|1088    |5013       |2021-07-22 00:00:00|676       |5       |389.54    |1947.70     |\n",
      "|4101    |4932       |2021-11-12 00:00:00|478       |4       |181.72    |726.88      |\n",
      "|3918    |5249       |2023-09-22 00:00:00|466       |2       |362.91    |725.82      |\n",
      "|3918    |5249       |2023-09-22 00:00:00|759       |3       |18.65     |55.95       |\n",
      "|9465    |8747       |2020-03-31 00:00:00|500       |1       |353.69    |353.69      |\n",
      "|6357    |2857       |2020-01-07 00:00:00|83        |4       |193.49    |773.96      |\n",
      "|4519    |1910       |2020-07-05 00:00:00|942       |3       |34.79     |104.37      |\n",
      "|496     |8193       |2025-02-13 00:00:00|468       |5       |155.46    |777.30      |\n",
      "|1342    |6669       |2022-02-01 00:00:00|822       |5       |322.50    |1612.50     |\n",
      "|8638    |5566       |2020-01-22 00:00:00|594       |2       |177.38    |354.76      |\n",
      "|9852    |3002       |2022-08-25 00:00:00|190       |5       |181.15    |905.75      |\n",
      "|496     |8193       |2025-02-13 00:00:00|88        |5       |254.05    |1270.25     |\n",
      "|4101    |4932       |2021-11-12 00:00:00|964       |4       |243.19    |972.76      |\n",
      "|496     |8193       |2025-02-13 00:00:00|189       |4       |71.79     |287.16      |\n",
      "|9465    |8747       |2020-03-31 00:00:00|801       |2       |44.54     |89.08       |\n",
      "|7168    |8930       |2021-02-16 00:00:00|999       |5       |82.96     |414.80      |\n",
      "|4190    |4229       |2021-06-20 00:00:00|469       |4       |385.79    |1543.16     |\n",
      "|243     |9916       |2024-06-16 00:00:00|28        |1       |443.67    |443.67      |\n",
      "|4190    |4229       |2021-06-20 00:00:00|704       |4       |357.40    |1429.60     |\n",
      "|7168    |8930       |2021-02-16 00:00:00|752       |5       |436.82    |2184.10     |\n",
      "+--------+-----------+-------------------+----------+--------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, col\n",
    "\n",
    "# Join orders with order_items to get product-level sales\n",
    "sales_data = order_df.join(order_items_df, \"order_id\", \"inner\") \\\n",
    "    .join(products_df, \"product_id\", \"inner\") \\\n",
    "    .select(\n",
    "        order_df.order_id,\n",
    "        order_df.customer_id,\n",
    "        order_df.order_date,\n",
    "        order_items_df.product_id,\n",
    "        order_items_df.quantity,\n",
    "        order_items_df.unit_price,\n",
    "        (order_items_df.quantity * order_items_df.unit_price).alias(\"sales_amount\")\n",
    "    )\n",
    "\n",
    "sales_data.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516d370-9b9a-4b48-a2b7-88dcfd7d5de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/03 13:05:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "historical_customer_sales_df=spark.read.parquet(\"s3://feb2025-training-bucket/RetailCustomer360/historical_sales_data\")\n",
    "sales_data = sales_data.withColumn(\"order_year\", year(col(\"order_date\"))) \\\n",
    "       .withColumn(\"order_month\", month(col(\"order_date\"))) \n",
    "#historical_customer_sales_df.count()\n",
    "sales_data = sales_data.union(historical_customer_sales_df)\n",
    "#historical_customer_sales_df.printSchema()\n",
    "#sales_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07086f3c-07d6-4a68-bedc-81bc72d1cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = sales_data.withColumn(\"order_year\", year(col(\"order_date\"))) \\\n",
    "       .withColumn(\"order_month\", month(col(\"order_date\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a66dc-bddf-46d2-ab81-24dc552eaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, sum, date_format, year, month\n",
    "# Extract Year, Month, and Quarter\n",
    "df = sales_data.withColumn(\"order_quarter\", date_format(col(\"order_date\"), \"Q\"))\n",
    "\n",
    "# Calculate Seasonal Trends\n",
    "# 1. Monthly Price Trend\n",
    "monthly_trends = df.groupBy(\"product_id\",\"order_year\", \"order_month\").agg(\n",
    "    avg(\"unit_price\").alias(\"avg_unit_price\"),\n",
    "    sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "    sum(\"quantity\").alias(\"total_qt\"),\n",
    ").orderBy(\"product_id\",\"order_year\", \"order_month\")\n",
    "\n",
    "# 2. Quarterly Price Trend\n",
    "quarterly_trends = df.groupBy(\"product_id\",\"order_year\", \"order_quarter\").agg(\n",
    "    avg(\"unit_price\").alias(\"avg_unit_price\"),\n",
    "    sum(\"sales_amount\").alias(\"total_sales\"),\n",
    "    sum(\"quantity\").alias(\"total_qt\"),\n",
    ").orderBy(\"product_id\",\"order_year\", \"order_quarter\")\n",
    "\n",
    "# Show Results\n",
    "monthly_trends.show()\n",
    "quarterly_trends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843aa265-948e-4c6c-9987-ab657c405741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AWS Glue database and table names\n",
    "glue_database = \"customer_analytics\"\n",
    "glue_table = \"monthly_trends\"\n",
    "\n",
    "# Define S3 output path\n",
    "s3_output_path = \"s3://feb2025-training-bucket/analytics/monthly_trends/\"\n",
    "\n",
    "# Create the AWS Glue Catalog table using the DataFrame\n",
    "monthly_trends.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", s3_output_path) \\\n",
    "    .saveAsTable(f\"{glue_database}.{glue_table}\")\n",
    "\n",
    "print(f\"Aggregated sales data written to S3: {s3_output_path}\")\n",
    "print(f\"Glue table '{glue_database}.{glue_table}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621252b-91ac-4162-bc03-393b1472b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "# Aggregate by customer_id\n",
    "customer_segmented = sales_data.groupBy(\"customer_id\").agg(\n",
    "    count(\"order_id\").alias(\"purchase_count\"),   # Total number of orders\n",
    "    sum(\"sales_amount\").alias(\"total_spent\"),max(\"order_date\").alias(\"last_order_date\")) # Most recent order date\n",
    "   # collect_list(\"product_id\").alias(\"product_ids\")  # List of purchased product IDs\n",
    "\n",
    "customer_segmented.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c52e0-5f8c-4bb4-baa7-e5be581b9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "customer_segmented = customer_segmented.withColumn(\n",
    "    \"customer_segment\",\n",
    "    when((col(\"purchase_count\") >= 10) & (col(\"total_spent\") >= 1000), \"High-Value Customer\")\n",
    "    .when((col(\"purchase_count\") >= 5) & (col(\"total_spent\") >= 500), \"Regular Customer\")\n",
    "    .otherwise(\"Occasional Buyer\")\n",
    ")\n",
    "\n",
    "customer_segmented.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350542b-b183-4f28-a0ce-66a547a5b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AWS Glue database and table names\n",
    "glue_database = \"customer_analytics\"\n",
    "glue_table = \"customer_segmented\"\n",
    "\n",
    "# Define S3 output path\n",
    "s3_output_path = \"s3://feb2025-training-bucket/analytics/customer_segmented/\"\n",
    "\n",
    "# Create the AWS Glue Catalog table using the DataFrame\n",
    "customer_segmented.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", s3_output_path) \\\n",
    "    .saveAsTable(f\"{glue_database}.{glue_table}\")\n",
    "\n",
    "print(f\"Aggregated sales data written to S3: {s3_output_path}\")\n",
    "print(f\"Glue table '{glue_database}.{glue_table}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9f19c-49c0-4816-b4e2-a0dff14f3abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
