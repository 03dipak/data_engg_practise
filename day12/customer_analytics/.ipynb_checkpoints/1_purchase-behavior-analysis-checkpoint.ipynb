{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea14ef1-360c-41ec-b787-ebfac825fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68517c44-9c5d-44a9-b4b2-994dae247f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-96fe0fb1-dd9e-43ec-b305-7f7e2fd426f0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 259ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-96fe0fb1-dd9e-43ec-b305-7f7e2fd426f0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/12ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 04:18:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/13 04:18:15 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"RetailCustomer360\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ab4934-dc82-4ada-ae83-fc56430c1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MySQL table into PySpark DataFrame\n",
    "customer_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "order_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "# Register as Temp Tables\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "order_df.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4b686e-d898-4196-bfa1-2f328682c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+-------------------+\n",
      "|customer_id|first_name|last_name|               email|               phone|             address|             city|        state|zip_code|         country|         created_at|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+-------------------+\n",
      "|          1|    Travis|     Long| randy93@example.net|  794.506.4087x24293|9032 Fitzgerald A...|        Lake Mark|     Nebraska|   94426|          Rwanda|2025-03-13 04:00:09|\n",
      "|          2|     Karen|    White|sarahmccullough@e...|+1-315-601-2749x7...|7159 Dennis Commo...|      Meaganmouth|West Virginia|   41653|     Puerto Rico|2025-03-13 04:00:11|\n",
      "|          3|     Kelly|    Baker|goodtammy@example...|     +1-974-603-9525|90282 Richard Gar...|       West Marie|      Florida|   69525|       Singapore|2025-03-13 04:00:09|\n",
      "|          4|    Justin|    Gibbs|steven73@example.com|001-471-802-2684x...|106 Christina Ter...|North Raymondside|   Washington|   98987|           Chile|2025-03-13 04:00:10|\n",
      "|          5|   Tiffany| Castillo|torresphilip@exam...|          9108870094|66392 Scott Trace...|      Natalieside|    Louisiana|   41800|         Lebanon|2025-03-13 04:00:07|\n",
      "|          6|   Richard|  English|  zbanks@example.com|   (265)379-2291x854|2931 Spears Walk\\...|   North Janefort|     Arkansas|   28876|         Algeria|2025-03-13 04:00:08|\n",
      "|          7|     Nancy|     Dunn|melissa26@example...|     +1-245-227-0312|5406 Michael Road...|       Shawnburgh|  Mississippi|   21122|        Anguilla|2025-03-13 04:00:08|\n",
      "|          8|      Lisa|  Johnson|popeshane@example...|        678-566-4253|0341 Blackwell Is...|    East Jennifer|      Georgia|   27767|          Cyprus|2025-03-13 04:00:11|\n",
      "|          9|     David|    Simon|maryortiz@example...|  (482)991-7008x1147|5014 Ramos Ridges...|    Port Seanport|     Maryland|   95543|        Guernsey|2025-03-13 04:00:07|\n",
      "|         10|     James|   Rivera|palmercrystal@exa...|    880.660.9777x191|146 Colin Skyway\\...|  Port Maxchester|New Hampshire|   28595|Marshall Islands|2025-03-13 04:00:09|\n",
      "|         11|     Laura| Mitchell|jennifer82@exampl...|+1-565-839-1297x8354|68232 Benjamin Is...|    Lake Mistyton|  Mississippi|   40498|        Barbados|2025-03-13 04:00:10|\n",
      "|         12|  Jennifer|   Miller|  hpetty@example.org|  (458)232-9711x9115|03209 Ricky Loaf\\...|      Bradfordton| South Dakota|   54076|            Iraq|2025-03-13 04:00:09|\n",
      "|         13|    Hailey|    Jones|kathleen08@exampl...|+1-743-529-3930x1...|6389 Emily Isle\\n...|         Halltown|        Idaho|   68142|        Malaysia|2025-03-13 04:00:10|\n",
      "|         14|    Gerald| Davidson|elizabethdavila@e...|       (619)564-9764|90418 Mcdonald Pl...|  South Bettystad|New Hampshire|   28850|         Comoros|2025-03-13 04:00:10|\n",
      "|         15|  Danielle|    Clark|tylersherri@examp...|        953-865-1862|228 Rodriguez Gre...|        Walshview|    Louisiana|   67377|           Congo|2025-03-13 04:00:09|\n",
      "|         16|  Jonathan|    Olson|robert89@example.com|001-414-594-3058x...|75072 Casey Brook...|   North Annabury|    Minnesota|   62906|      Cape Verde|2025-03-13 04:00:10|\n",
      "|         17|   Valerie|    Gomez|daniellegarcia@ex...|    912-870-3865x625|86501 Mark Island...|Lake Timothyshire|     Arkansas|   18752|        Cambodia|2025-03-13 04:00:11|\n",
      "|         18|    Thomas|  Walters|nicole96@example.org|    001-946-930-5883|43949 Angela Lock...|     Port Barbara|West Virginia|   14370|     Philippines|2025-03-13 04:00:07|\n",
      "|         19|    Teresa|Armstrong|anthonynorris@exa...|          9929222232|1371 Kevin Trail\\...|       Karenmouth|     Nebraska|   07152|      Azerbaijan|2025-03-13 04:00:11|\n",
      "|         20|   William|    Perry|thomaslori@exampl...|  689.275.4845x89859|594 Moran Burgs A...|       North Adam|       Alaska|   58794|     Timor-Leste|2025-03-13 04:00:11|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133c907a-d105-4925-a326-84ea17dee80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+---------+------------+\n",
      "|order_id|customer_id|         order_date|   status|total_amount|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "|    4372|        553|2021-05-22 00:00:00|Delivered|      270.97|\n",
      "|   11935|        553|2021-04-22 00:00:00|  Pending|      362.66|\n",
      "|   12905|        553|2023-04-29 00:00:00|Cancelled|      492.41|\n",
      "|   18522|        553|2024-07-18 00:00:00|  Pending|      362.69|\n",
      "|   19907|        553|2022-09-23 00:00:00|Cancelled|      273.38|\n",
      "|   24646|        553|2022-07-13 00:00:00|Delivered|      360.91|\n",
      "|   27633|        553|2024-03-11 00:00:00|  Pending|      483.50|\n",
      "|   30102|        553|2023-03-24 00:00:00|Delivered|      480.41|\n",
      "|   30778|        553|2021-10-22 00:00:00|Cancelled|      378.43|\n",
      "|   44156|        553|2021-04-04 00:00:00|Cancelled|      400.85|\n",
      "|   51296|        553|2025-01-22 00:00:00|  Pending|       13.93|\n",
      "|   68023|        553|2020-11-17 00:00:00|Delivered|      395.71|\n",
      "|   78095|        553|2023-12-16 00:00:00|  Shipped|       56.75|\n",
      "|   78189|        553|2020-12-24 00:00:00|  Pending|      345.11|\n",
      "|   78727|        553|2021-01-21 00:00:00|Delivered|      445.64|\n",
      "|   82090|        553|2022-08-18 00:00:00|Cancelled|      435.17|\n",
      "|   86186|        553|2022-08-16 00:00:00|Delivered|      369.56|\n",
      "|   94414|        553|2022-10-11 00:00:00|  Pending|      497.43|\n",
      "|   94973|        553|2020-08-23 00:00:00|Cancelled|      271.66|\n",
      "|   95289|        553|2020-08-05 00:00:00|Delivered|      184.22|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *   FROM orders o  where o.customer_id=553 \"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1c3429-acc6-4d94-adc9-7cae792f5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|        553|    6881.39|\n",
      "|       6188|    6646.06|\n",
      "|       6940|    6545.92|\n",
      "|       5361|    6519.63|\n",
      "|       7440|    6292.82|\n",
      "|       8459|    6152.89|\n",
      "|       1023|    6071.49|\n",
      "|       6208|    6063.56|\n",
      "|       4607|    6030.39|\n",
      "|       7658|    5993.29|\n",
      "|         70|    5957.60|\n",
      "|       6305|    5912.17|\n",
      "|       7153|    5897.98|\n",
      "|       5231|    5853.98|\n",
      "|       1821|    5799.72|\n",
      "|       2667|    5797.99|\n",
      "|       1524|    5790.05|\n",
      "|       2242|    5766.12|\n",
      "|       6724|    5764.37|\n",
      "|       3469|    5670.20|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent\n",
    "        FROM orders o GROUP BY o.customer_id  order by total_spent desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190428bf-3f49-49c1-a603-baf117748d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-------------------+\n",
      "|customer_id|total_spent|total_orders| last_purchase_date|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "|        553|    6881.39|          20|2025-01-22 00:00:00|\n",
      "|       6188|    6646.06|          20|2024-09-27 00:00:00|\n",
      "|       6940|    6545.92|          20|2025-02-23 00:00:00|\n",
      "|       5361|    6519.63|          20|2024-11-18 00:00:00|\n",
      "|       7440|    6292.82|          21|2024-09-24 00:00:00|\n",
      "|       8459|    6152.89|          22|2024-12-26 00:00:00|\n",
      "|       1023|    6071.49|          20|2024-09-11 00:00:00|\n",
      "|       6208|    6063.56|          20|2024-11-09 00:00:00|\n",
      "|       4607|    6030.39|          19|2024-12-01 00:00:00|\n",
      "|       7658|    5993.29|          20|2025-01-21 00:00:00|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_customers = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o \n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc  \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570d5292-2fb0-4944-8b31-b8949fec3638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|       country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Kingdom|         70| Stephanie|donnajones@exampl...|    1251.89|           4|2024-09-28 00:00:00|            1|\n",
      "|United Kingdom|       2185|   Tiffany|johnsonjennifer@e...|    1234.75|           4|2025-02-18 00:00:00|            2|\n",
      "|United Kingdom|       7597|     Tanya|  iwoods@example.org|    1221.33|           3|2024-12-18 00:00:00|            3|\n",
      "|United Kingdom|       4895|  Jennifer|emilymanning@exam...|     917.19|           3|2024-12-14 00:00:00|            4|\n",
      "|United Kingdom|       4264| Christian|tfleming@example.net|     843.86|           3|2024-09-21 00:00:00|            5|\n",
      "|United Kingdom|        402|     Kevin|christinaadkins@e...|     820.48|           4|2025-03-10 00:00:00|            6|\n",
      "|United Kingdom|       3666|   Anthony|justin26@example.net|     806.65|           3|2025-02-05 00:00:00|            7|\n",
      "|United Kingdom|       2731|     Bruce|johnmoore@example...|     802.29|           4|2025-03-07 00:00:00|            8|\n",
      "|United Kingdom|       9974|    Robert|amanda58@example.net|     745.25|           2|2024-11-12 00:00:00|            9|\n",
      "|United Kingdom|       4856|     Wendy|jeffrey97@example...|     655.46|           2|2024-03-16 00:00:00|           10|\n",
      "|United Kingdom|       3254|    Daniel| ldaniel@example.org|     612.16|           3|2025-02-18 00:00:00|           11|\n",
      "|United Kingdom|       7385|    Edward|   ypope@example.com|     576.72|           2|2025-01-24 00:00:00|           12|\n",
      "|United Kingdom|       1450|     James|blakechristopher@...|     401.92|           1|2024-08-08 00:00:00|           13|\n",
      "|United Kingdom|       9837|  Jennifer|stephanie67@examp...|     377.55|           3|2025-01-01 00:00:00|           14|\n",
      "|United Kingdom|       6934|     Jason|josephgilbert@exa...|     333.04|           2|2024-12-08 00:00:00|           15|\n",
      "|United Kingdom|       9692|      Sara|briansanchez@exam...|     284.16|           1|2024-10-02 00:00:00|           16|\n",
      "|United Kingdom|       1671|     James|ssanchez@example.org|     258.46|           1|2025-01-08 00:00:00|           17|\n",
      "|United Kingdom|       9536|      Lisa| frank03@example.net|     233.59|           2|2024-04-12 00:00:00|           18|\n",
      "|United Kingdom|        934|    Hayden|markrivera@exampl...|     211.86|           1|2025-01-13 00:00:00|           19|\n",
      "|United Kingdom|       7252|     Kevin|  lisa07@example.com|     152.86|           1|2024-06-07 00:00:00|           20|\n",
      "|United Kingdom|       2189|   Tiffany|farrelldanny@exam...|     138.13|           1|2024-10-13 00:00:00|           21|\n",
      "|United Kingdom|       4930|    Cheryl|campbellsean@exam...|      39.63|           1|2025-02-01 00:00:00|           22|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)   -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc \n",
    "    )\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER ( PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id where c.country='United Kingdom'  GROUP BY c.country,c.customer_id,c.first_name,c.email,cs.total_spent,cs.last_purchase_date,cs.total_orders order by total_spent desc\n",
    "\"\"\").show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "958dee8b-983b-4384-86e0-befda5cbddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|             country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Arab Emirates|       5354|      Ryan|lisamorrison@exam...|    1565.06|           5|2025-02-27 00:00:00|            1|\n",
      "|United Arab Emirates|       6218| Katherine|walkerrobert@exam...|    1473.88|           4|2024-10-15 00:00:00|            2|\n",
      "|United Arab Emirates|       8647|  Danielle|  bsmith@example.com|    1367.50|           5|2025-02-11 00:00:00|            3|\n",
      "|United Arab Emirates|        110|  Samantha|stephenobrien@exa...|    1218.35|           4|2024-11-09 00:00:00|            4|\n",
      "|United Arab Emirates|       5727|     Kevin| david25@example.com|    1153.08|           4|2025-01-25 00:00:00|            5|\n",
      "|United Arab Emirates|       6464|     Sarah|traceyyoung@examp...|    1126.01|           4|2024-11-28 00:00:00|            6|\n",
      "|United Arab Emirates|       9201| Christina|   xcarr@example.com|    1064.73|           4|2025-03-01 00:00:00|            7|\n",
      "|United Arab Emirates|        750|    Joanne|mhernandez@exampl...|    1042.31|           3|2024-12-24 00:00:00|            8|\n",
      "|United Arab Emirates|       2524|  Samantha|mackmiguel@exampl...|     909.08|           3|2025-03-12 00:00:00|            9|\n",
      "|United Arab Emirates|       6765|     Robin|  ohenry@example.com|     860.18|           3|2024-06-29 00:00:00|           10|\n",
      "|      United Kingdom|         70| Stephanie|donnajones@exampl...|    1251.89|           4|2024-09-28 00:00:00|            1|\n",
      "|      United Kingdom|       2185|   Tiffany|johnsonjennifer@e...|    1234.75|           4|2025-02-18 00:00:00|            2|\n",
      "|      United Kingdom|       7597|     Tanya|  iwoods@example.org|    1221.33|           3|2024-12-18 00:00:00|            3|\n",
      "|      United Kingdom|       4895|  Jennifer|emilymanning@exam...|     917.19|           3|2024-12-14 00:00:00|            4|\n",
      "|      United Kingdom|       4264| Christian|tfleming@example.net|     843.86|           3|2024-09-21 00:00:00|            5|\n",
      "|      United Kingdom|        402|     Kevin|christinaadkins@e...|     820.48|           4|2025-03-10 00:00:00|            6|\n",
      "|      United Kingdom|       3666|   Anthony|justin26@example.net|     806.65|           3|2025-02-05 00:00:00|            7|\n",
      "|      United Kingdom|       2731|     Bruce|johnmoore@example...|     802.29|           4|2025-03-07 00:00:00|            8|\n",
      "|      United Kingdom|       9974|    Robert|amanda58@example.net|     745.25|           2|2024-11-12 00:00:00|            9|\n",
      "|      United Kingdom|       4856|     Wendy|jeffrey97@example...|     655.46|           2|2024-03-16 00:00:00|           10|\n",
      "|United States Min...|       9751|    Robert|miranda58@example...|    1485.21|           4|2025-01-01 00:00:00|            1|\n",
      "|United States Min...|       8089|     Donna|diamond79@example...|    1343.53|           4|2025-02-22 00:00:00|            2|\n",
      "|United States Min...|       2918|     Peter|christine02@examp...|    1305.35|           3|2025-03-06 00:00:00|            3|\n",
      "|United States Min...|       9004|     Erika|paulprince@exampl...|    1093.32|           3|2025-02-25 00:00:00|            4|\n",
      "|United States Min...|       6977|     Jimmy|daniel38@example.com|     859.77|           2|2024-09-03 00:00:00|            5|\n",
      "|United States Min...|       1290|    Ashley|leslie12@example.org|     800.48|           2|2025-02-24 00:00:00|            6|\n",
      "|United States Min...|       5883|   Colleen|   tbond@example.net|     766.58|           2|2025-02-10 00:00:00|            7|\n",
      "|United States Min...|       1288|     Derek|theresa48@example...|     753.53|           3|2025-03-02 00:00:00|            8|\n",
      "|United States Min...|       3997|     Sarah| linda06@example.com|     743.42|           2|2025-01-06 00:00:00|            9|\n",
      "|United States Min...|       3512|      Gina|lorettasmith@exam...|     672.64|           4|2025-01-10 00:00:00|           10|\n",
      "|United States Vir...|        634|      Erik|alexmayo@example.org|    1880.66|           6|2025-02-17 00:00:00|            1|\n",
      "|United States Vir...|       3790|    Edward|christinabarnes@e...|    1701.86|           7|2025-02-12 00:00:00|            2|\n",
      "|United States Vir...|       8472|    Andrew| uingram@example.com|    1271.24|           4|2025-03-02 00:00:00|            3|\n",
      "|United States Vir...|        736|   Jeffrey|penadanielle@exam...|    1109.54|           4|2025-01-07 00:00:00|            4|\n",
      "|United States Vir...|       1401|  Savannah|quinnmarie@exampl...|     938.41|           3|2024-12-29 00:00:00|            5|\n",
      "|United States Vir...|       9923|     Mandy| gnewman@example.com|     781.31|           2|2025-02-06 00:00:00|            6|\n",
      "|United States Vir...|        643|    Dennis|tracywright@examp...|     766.40|           3|2025-03-02 00:00:00|            7|\n",
      "|United States Vir...|        243|   Kaitlyn|jamesmoore@exampl...|     753.64|           3|2025-01-17 00:00:00|            8|\n",
      "|United States Vir...|       3069| Stephanie|jenniferrowe@exam...|     742.46|           4|2025-01-15 00:00:00|            9|\n",
      "|United States Vir...|       7666|    Robert|kennethmills@exam...|     601.33|           2|2025-01-06 00:00:00|           10|\n",
      "|United States of ...|       5891|   Michael|alexander79@examp...|    1627.40|           4|2025-02-14 00:00:00|            1|\n",
      "|United States of ...|       8157|     Kevin|garciamichael@exa...|    1237.42|           4|2025-02-28 00:00:00|            2|\n",
      "|United States of ...|        295|   Rebecca|tgonzalez@example...|    1177.23|           3|2024-07-14 00:00:00|            3|\n",
      "|United States of ...|        770|  Jennifer| ubailey@example.net|    1091.77|           3|2024-12-20 00:00:00|            4|\n",
      "|United States of ...|       5315|    Kelsey|brookeknox@exampl...|    1090.99|           3|2025-03-06 00:00:00|            5|\n",
      "|United States of ...|        950|  Nicholas|gilbertzachary@ex...|    1001.88|           5|2025-02-13 00:00:00|            6|\n",
      "|United States of ...|       1600|   Natalie| ogibson@example.com|     927.53|           3|2024-10-09 00:00:00|            7|\n",
      "|United States of ...|       8129|      Mark|jensenstephanie@e...|     917.18|           2|2025-01-03 00:00:00|            8|\n",
      "|United States of ...|       6415|    Thomas| rhester@example.com|     722.75|           3|2024-11-13 00:00:00|            9|\n",
      "|United States of ...|       6925|    Dustin|michelletorres@ex...|     700.75|           2|2024-06-15 00:00:00|           10|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Spark SQL Query\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id\n",
    "    ),\n",
    "    customer_ranking AS (\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER (PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id\n",
    "    )\n",
    "    SELECT * FROM customer_ranking WHERE spending_rank <= 10 and country like 'United %';\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "top_customers.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b9c2e-8f02-4d45-beb5-94528c3ecbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Filter last 1 year of data\n",
    "one_year_ago = F.date_add(F.current_date(), -365)\n",
    "filtered_orders = order_df.filter(F.col(\"order_date\") >= one_year_ago)\n",
    "\n",
    "# Aggregate customer spending\n",
    "customer_spending = (\n",
    "    filtered_orders.groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.max(\"order_date\").alias(\"last_purchase_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with customers table\n",
    "customer_data = customer_spending.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Define window specification for ranking\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.desc(\"total_spent\"))\n",
    "\n",
    "# Add ranking column\n",
    "customer_ranking = customer_data.withColumn(\"spending_rank\", F.rank().over(window_spec))\n",
    "\n",
    "# Filter top 100 customers\n",
    "top_customers = customer_ranking.filter( (F.col(\"spending_rank\") <= 10) &  (F.col(\"country\").like(\"United %\")))\n",
    "\n",
    "# Show results\n",
    "top_customers.select(\"country\",\"customer_id\",\"first_name\",\"email\",\"total_spent\",\"total_orders\",\"spending_rank\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3abae0a8-6c0d-4787-87bc-bffa5522979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 04:38:47 INFO HiveConf: Found configuration file file:/home/glue_user/spark/conf/hive-site.xml\n",
      "25/03/13 04:38:48 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/13 04:38:48 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/13 04:38:48 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/13 04:38:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/13 04:39:02 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=8aef4fb0-cd4c-4909-9690-b322e086b826, clientType=HIVECLI]\n",
      "25/03/13 04:39:02 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/03/13 04:39:02 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "25/03/13 04:39:02 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/13 04:39:02 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/13 04:39:02 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/13 04:39:02 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/13 04:39:02 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/13 04:39:02 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sales data written to S3: s3://feb2025-training-bucket/analytics/top_100_customers/\n",
      "Glue table 'customer_analytics.top_100_customers' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define AWS Glue database and table names\n",
    "glue_database = \"customer_analytics\"\n",
    "glue_table = \"top_100_customers\"\n",
    "\n",
    "# Define S3 output path\n",
    "s3_output_path = \"s3://feb2025-training-bucket/analytics/top_100_customers/\"\n",
    "\n",
    "# Create the AWS Glue Catalog table using the DataFrame\n",
    "top_customers.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", s3_output_path) \\\n",
    "    .saveAsTable(f\"{glue_database}.{glue_table}\")\n",
    "\n",
    "print(f\"Aggregated sales data written to S3: {s3_output_path}\")\n",
    "print(f\"Glue table '{glue_database}.{glue_table}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd734bb2-b506-49cc-a469-f69517a718a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
