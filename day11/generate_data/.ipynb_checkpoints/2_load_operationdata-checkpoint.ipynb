{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bf541-3371-4469-b3a1-a76449c055f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d0e0f-4fda-412e-a6e8-1711b93fb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9258e1-b5d2-4526-8b8a-4b214bf4e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-32e69753-d2c5-46c5-8f9d-1b80fc4dd0c2;1.0\n",
      "\tconfs: [default]\n",
      "mysql#mysql-connector-java;8.0.33 is relocated to com.mysql#mysql-connector-j;8.0.33. Please update your dependencies.\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "downloading https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.33/mysql-connector-j-8.0.33.jar ...\n",
      "\t[SUCCESSFUL ] com.mysql#mysql-connector-j;8.0.33!mysql-connector-j.jar (1595ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.21.9/protobuf-java-3.21.9.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.21.9!protobuf-java.jar(bundle) (632ms)\n",
      ":: resolution report :: resolve 6056ms :: artifacts dl 2233ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-32e69753-d2c5-46c5-8f9d-1b80fc4dd0c2\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (4055kB/37ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"load_operation_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403fbdd2-43bb-41cb-bfe3-7d836723017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker for realistic data\n",
    "fake = Faker()\n",
    "\n",
    "# Generate Customers\n",
    "def generate_customers(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.first_name(), fake.last_name(), fake.email(), fake.phone_number(),\n",
    "          fake.address(), fake.city(), fake.state(), fake.zipcode(), fake.country()) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"zip_code\", \"country\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c893ab-171b-49e9-8825-fb2abfec9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+\n",
      "|customer_id|first_name|last_name|               email|               phone|             address|             city|        state|zip_code|         country|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+\n",
      "|          1|    Travis|     Long| randy93@example.net|  794.506.4087x24293|9032 Fitzgerald A...|        Lake Mark|     Nebraska|   94426|          Rwanda|\n",
      "|          2|     Karen|    White|sarahmccullough@e...|+1-315-601-2749x7...|7159 Dennis Commo...|      Meaganmouth|West Virginia|   41653|     Puerto Rico|\n",
      "|          3|     Kelly|    Baker|goodtammy@example...|     +1-974-603-9525|90282 Richard Gar...|       West Marie|      Florida|   69525|       Singapore|\n",
      "|          4|    Justin|    Gibbs|steven73@example.com|001-471-802-2684x...|106 Christina Ter...|North Raymondside|   Washington|   98987|           Chile|\n",
      "|          5|   Tiffany| Castillo|torresphilip@exam...|          9108870094|66392 Scott Trace...|      Natalieside|    Louisiana|   41800|         Lebanon|\n",
      "|          6|   Richard|  English|  zbanks@example.com|   (265)379-2291x854|2931 Spears Walk\\...|   North Janefort|     Arkansas|   28876|         Algeria|\n",
      "|          7|     Nancy|     Dunn|melissa26@example...|     +1-245-227-0312|5406 Michael Road...|       Shawnburgh|  Mississippi|   21122|        Anguilla|\n",
      "|          8|      Lisa|  Johnson|popeshane@example...|        678-566-4253|0341 Blackwell Is...|    East Jennifer|      Georgia|   27767|          Cyprus|\n",
      "|          9|     David|    Simon|maryortiz@example...|  (482)991-7008x1147|5014 Ramos Ridges...|    Port Seanport|     Maryland|   95543|        Guernsey|\n",
      "|         10|     James|   Rivera|palmercrystal@exa...|    880.660.9777x191|146 Colin Skyway\\...|  Port Maxchester|New Hampshire|   28595|Marshall Islands|\n",
      "|         11|     Laura| Mitchell|jennifer82@exampl...|+1-565-839-1297x8354|68232 Benjamin Is...|    Lake Mistyton|  Mississippi|   40498|        Barbados|\n",
      "|         12|  Jennifer|   Miller|  hpetty@example.org|  (458)232-9711x9115|03209 Ricky Loaf\\...|      Bradfordton| South Dakota|   54076|            Iraq|\n",
      "|         13|    Hailey|    Jones|kathleen08@exampl...|+1-743-529-3930x1...|6389 Emily Isle\\n...|         Halltown|        Idaho|   68142|        Malaysia|\n",
      "|         14|    Gerald| Davidson|elizabethdavila@e...|       (619)564-9764|90418 Mcdonald Pl...|  South Bettystad|New Hampshire|   28850|         Comoros|\n",
      "|         15|  Danielle|    Clark|tylersherri@examp...|        953-865-1862|228 Rodriguez Gre...|        Walshview|    Louisiana|   67377|           Congo|\n",
      "|         16|  Jonathan|    Olson|robert89@example.com|001-414-594-3058x...|75072 Casey Brook...|   North Annabury|    Minnesota|   62906|      Cape Verde|\n",
      "|         17|   Valerie|    Gomez|daniellegarcia@ex...|    912-870-3865x625|86501 Mark Island...|Lake Timothyshire|     Arkansas|   18752|        Cambodia|\n",
      "|         18|    Thomas|  Walters|nicole96@example.org|    001-946-930-5883|43949 Angela Lock...|     Port Barbara|West Virginia|   14370|     Philippines|\n",
      "|         19|    Teresa|Armstrong|anthonynorris@exa...|          9929222232|1371 Kevin Trail\\...|       Karenmouth|     Nebraska|   07152|      Azerbaijan|\n",
      "|         20|   William|    Perry|thomaslori@exampl...|  689.275.4845x89859|594 Moran Burgs A...|       North Adam|       Alaska|   58794|     Timor-Leste|\n",
      "+-----------+----------+---------+--------------------+--------------------+--------------------+-----------------+-------------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate Data\n",
    "customers_df = generate_customers(10000)\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684edff-d8d8-4e71-a3b2-611510cb17bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fae67-1777-4f90-91bd-00e7c87f9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Products\n",
    "def generate_products(n):\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Home & Kitchen\", \"Books\", \"Toys\", \"Sports\"]\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.word().capitalize(), fake.sentence(nb_words=6), \n",
    "          random.choice(categories), round(random.uniform(5, 500), 2), random.randint(10, 1000)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"product_id\", \"name\", \"description\", \"category\", \"price\", \"stock_quantity\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Payments\n",
    "def generate_payments(n, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_order_id), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n, max_customer_id, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n, max_customer_id, max_product_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_product_id), \n",
    "          fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n, max_customer_id, max_order_id, max_payment_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          random.randint(1, max_payment_id), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678465d-efb9-4285-9fc4-1df92b496282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c6b21-7983-4c41-9c99-b576645bdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate emails before writing to MySQL\n",
    "customers_df = customers_df.dropDuplicates([\"email\"])\n",
    "\n",
    "customers_df.write.jdbc(url=USER_MYSQL_URL, table=\"Customers\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e42e6-fc97-4c45-b6be-d697c505126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = generate_products(1000)\n",
    "\n",
    "products_df.write.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86dde5-8a0b-4b1b-bc7e-a0487b7f70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customers_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "\n",
    "# ✅ Generate Orders using only valid customer IDs\n",
    "def generate_orders(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Pending\", \"Shipped\", \"Delivered\", \"Cancelled\"]), \n",
    "          round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_id\", \"customer_id\", \"order_date\", \"status\", \"total_amount\"]\n",
    "    )\n",
    "\n",
    "orders_df = generate_orders(100000)\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddd3d9-b125-46b3-91f7-084c438f86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d8144-9234-4f07-8872-1af7e64d46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#orders_df.write.jdbc(url=MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "orders_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "valid_order_ids = [row[\"order_id\"] for row in orders_df.select(\"order_id\").collect()]\n",
    "valid_product_ids = [row[\"product_id\"] for row in products_df.select(\"product_id\").collect()]\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "# ✅ Generate Order Items using only valid order_id & product_id\n",
    "def generate_order_items(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_order_ids), random.choice(valid_product_ids), \n",
    "          random.randint(1, 5), round(random.uniform(5, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_item_id\", \"order_id\", \"product_id\", \"quantity\", \"unit_price\"]\n",
    "    )\n",
    "order_items_df = generate_order_items(100000)\n",
    "order_items_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Order_Items\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed681e4f-a7db-4b96-aed9-0d950de2e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Payments\n",
    "def generate_payments(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_order_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "payments_df = generate_payments(10000)\n",
    "valid_payments_ids = [row[\"payment_id\"] for row in payments_df.select(\"payment_id\").collect()]\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "payments_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Payments\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "purchase_history_df = generate_purchase_history(1000)\n",
    "purchase_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Purchase_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8499c8-bf7a-4b00-bbf6-a38667e40133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids),random.choice(valid_product_ids),fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n,):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          random.choice(valid_payments_ids), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "usage_history_df = generate_usage_history(1000)\n",
    "usage_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Usage_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "transactions_df = generate_transactions(1000)\n",
    "transactions_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Transactions\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069b0ab-89a1-4102-9c60-4c639a9b8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,rand,lit\n",
    "# Generate random login data\n",
    "def generate_logins(customers_df, num_attempts=50):\n",
    "    logins = []\n",
    "    for row in customers_df.collect():\n",
    "        customer_id = row[\"customer_id\"]\n",
    "        num_logins = random.randint(1, num_attempts)  # Random login attempts per customer\n",
    "        \n",
    "        for _ in range(num_logins):\n",
    "            ip = f\"192.168.{random.randint(0, 255)}.{random.randint(0, 255)}\"\n",
    "            login_attempt = random.choice([\"SUCCESS\", \"FAILURE\"])\n",
    "            login_date = f\"2024-02-{random.randint(1, 28)}\"\n",
    "            \n",
    "            logins.append((customer_id, ip, login_attempt, login_date))\n",
    "    \n",
    "    return logins\n",
    "\n",
    "# Generate synthetic login data\n",
    "login_data = generate_logins(customers_df)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"customer_id\", \"ip_address\", \"login_attempt\", \"login_date\"]\n",
    "login_df = spark.createDataFrame(login_data, columns)\n",
    "\n",
    "# Introduce suspicious activity (customers with >3 IPs or >10 attempts)\n",
    "login_df = login_df.withColumn(\"is_suspicious\", when((rand() < 0.2), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Write to MySQL (optional)\n",
    "login_df.write.jdbc(url=USER_MYSQL_URL, table=\"LoginHistory\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "# Show result\n",
    "login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2696a-9227-43f4-939b-b782bf27dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
