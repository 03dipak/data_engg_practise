{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751bf541-3371-4469-b3a1-a76449c055f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1d0e0f-4fda-412e-a6e8-1711b93fb1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faker\n",
      "  Downloading faker-37.0.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzdata\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, faker\n",
      "Successfully installed faker-37.0.0 tzdata-2025.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9258e1-b5d2-4526-8b8a-4b214bf4e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1136d83c-cabb-4bee-affc-5e62ae5f6c99;1.0\n",
      "\tconfs: [default]\n",
      "mysql#mysql-connector-java;8.0.33 is relocated to com.mysql#mysql-connector-j;8.0.33. Please update your dependencies.\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "downloading https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.0.33/mysql-connector-j-8.0.33.jar ...\n",
      "\t[SUCCESSFUL ] com.mysql#mysql-connector-j;8.0.33!mysql-connector-j.jar (2397ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.21.9/protobuf-java-3.21.9.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.21.9!protobuf-java.jar(bundle) (1336ms)\n",
      ":: resolution report :: resolve 21546ms :: artifacts dl 3856ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1136d83c-cabb-4bee-affc-5e62ae5f6c99\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (4055kB/442ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"load_operation_data\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403fbdd2-43bb-41cb-bfe3-7d836723017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Faker for realistic data\n",
    "fake = Faker()\n",
    "\n",
    "# Generate Customers\n",
    "def generate_customers(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.first_name(), fake.last_name(), fake.email(), fake.phone_number(),\n",
    "          fake.address(), fake.city(), fake.state(), fake.zipcode(), fake.country()) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"address\", \"city\", \"state\", \"zip_code\", \"country\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c893ab-171b-49e9-8825-fb2abfec9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+\n",
      "|customer_id| first_name|last_name|               email|               phone|             address|               city|        state|zip_code|         country|\n",
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+\n",
      "|          1|    Michael|  Rodgers|katiecasey@exampl...|+1-664-301-3316x9...|USS Dudley\\nFPO A...|        North Traci|        Idaho|   38917|     Saint Lucia|\n",
      "|          2|      Paula|    Berry|carolyn15@example...|  971-396-8026x98919|PSC 9301, Box 951...|       Hornechester|     Michigan|   14169|          Turkey|\n",
      "|          3|     Angela| Lawrence|  kchung@example.org|   616-578-2644x4229|884 Brandi Pike A...|         New Kelsey|     Maryland|   38467|         Lesotho|\n",
      "|          4|      Nancy|      May| carol17@example.org|   823-433-0320x8126|254 Watson Via Su...|       Andersonport|     Arkansas|   38645|         Vietnam|\n",
      "|          5|Christopher|   Howell|melanie13@example...|+1-585-711-2536x7...|1818 William Gate...|       Spencerhaven|     New York|   64962|         Bahamas|\n",
      "|          6|     Robert|  Sanchez|bishopmelissa@exa...|  (971)730-0494x4499|3114 White Port S...|           New Erin|         Iowa|   20071|         Mayotte|\n",
      "|          7|     Joseph|      Kim|  owatts@example.org|     +1-834-620-5485|67994 Smith Light...|      Lake Paultown|     Missouri|   80184|           Samoa|\n",
      "|          8|     Robert|   Holmes|speterson@example...|   (265)435-4021x622|025 Larson Cove S...|South Cassandrafurt|     Virginia|   67006|            Togo|\n",
      "|          9|    Raymond| Robinson|   shart@example.com|  984.454.7585x90723|USNS Miller\\nFPO ...| North Melissahaven|     New York|   95746|         Nigeria|\n",
      "|         10|    Michael|    Meyer|bethbeasley@examp...|        623.900.0817|67351 Jennifer Po...|   New Michaelmouth|    Wisconsin|   02747|     Timor-Leste|\n",
      "|         11|       Erin|    Bruce|morrisonsteven@ex...|        386.671.8774|339 Frederick Mou...|      Lake Virginia|    Tennessee|   49628|            Guam|\n",
      "|         12|     Joanna|Patterson|boyertrevor@examp...|   386.602.4277x8614|27315 David Track...|        Curtisburgh|      Indiana|   47235|    Saudi Arabia|\n",
      "|         13|     Sandra|    Green|brandy26@example.com|   (964)384-1129x148|803 Danielle Plai...|         Sarahville|     Colorado|   21162|            Niue|\n",
      "|         14|  Elizabeth|    White|paulasmith@exampl...|001-668-382-8172x...|23430 Joseph Lock...|          Kevinfurt|     Colorado|   78721|       Hong Kong|\n",
      "|         15|    Rebecca|  Robbins|imontgomery@examp...|    566.929.3785x722|345 Ashley Cove\\n...|        East Thomas| South Dakota|   02378|      Mozambique|\n",
      "|         16|    Melissa|   Jordan|dzimmerman@exampl...|    391-346-4047x786|9879 Smith Juncti...|  Lake Davidchester|       Alaska|   80105|          Jordan|\n",
      "|         17|    Timothy|  Griffin|  yfrank@example.net|        542-579-3866|62163 Jodi View S...|        Jacksonberg|West Virginia|   17423|            Mali|\n",
      "|         18|    Patrick|    Davis|rmartinez@example...|  (414)470-1021x7542|94543 Gutierrez T...| North Briannamouth|      Vermont|   22272|Marshall Islands|\n",
      "|         19|       Kara|   Harvey|butleradrian@exam...| (665)460-5980x89623|63257 Larry Trail...|       Russellshire|     Oklahoma|   13607|      San Marino|\n",
      "|         20|      Scott|     King| holly99@example.org|   302.957.4187x3470|2212 Dawson Club\\...|        Riveramouth|   New Jersey|   06081|    South Africa|\n",
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate Data\n",
    "customers_df = generate_customers(1000)\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a684edff-d8d8-4e71-a3b2-611510cb17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================> (194 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to MySQL!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove duplicate emails before writing to MySQL\n",
    "customers_df = customers_df.dropDuplicates([\"email\"])\n",
    "\n",
    "customers_df.write.jdbc(url=USER_MYSQL_URL, table=\"Customers\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315fae67-1777-4f90-91bd-00e7c87f9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Products\n",
    "def generate_products(n):\n",
    "    categories = [\"Electronics\", \"Clothing\", \"Home & Kitchen\", \"Books\", \"Toys\", \"Sports\"]\n",
    "    return spark.createDataFrame(\n",
    "        [(i, fake.word().capitalize(), fake.sentence(nb_words=6), \n",
    "          random.choice(categories), round(random.uniform(5, 500), 2), random.randint(10, 1000)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"product_id\", \"name\", \"description\", \"category\", \"price\", \"stock_quantity\"]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate Payments\n",
    "def generate_payments(n, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_order_id), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n, max_customer_id, max_order_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n, max_customer_id, max_product_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_product_id), \n",
    "          fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n, max_customer_id, max_order_id, max_payment_id):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.randint(1, max_customer_id), random.randint(1, max_order_id), \n",
    "          random.randint(1, max_payment_id), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678465d-efb9-4285-9fc4-1df92b496282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1e42e6-fc97-4c45-b6be-d697c505126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_df = generate_products(1000)\n",
    "\n",
    "products_df.write.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf86dde5-8a0b-4b1b-bc7e-a0487b7f70b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+---------+------------+\n",
      "|order_id|customer_id|order_date|   status|total_amount|\n",
      "+--------+-----------+----------+---------+------------+\n",
      "|       1|         68|2022-07-22|  Shipped|      236.06|\n",
      "|       2|        458|2021-09-13|Cancelled|       191.2|\n",
      "|       3|        158|2021-03-12|  Shipped|      227.43|\n",
      "|       4|        199|2020-10-18|Cancelled|      357.54|\n",
      "|       5|        989|2021-06-28|  Pending|       72.94|\n",
      "|       6|        418|2021-01-28|  Shipped|      108.65|\n",
      "|       7|        379|2021-03-18|Delivered|      247.31|\n",
      "|       8|        727|2020-08-13|  Pending|       59.05|\n",
      "|       9|        648|2020-05-09|Delivered|      319.87|\n",
      "|      10|        112|2024-07-11|  Pending|      183.45|\n",
      "|      11|        455|2023-05-14|  Shipped|       24.82|\n",
      "|      12|        123|2024-07-12|  Shipped|      309.55|\n",
      "|      13|        655|2024-08-22|  Shipped|      396.34|\n",
      "|      14|        938|2020-06-23|Delivered|      492.61|\n",
      "|      15|        452|2023-04-04|  Shipped|      218.48|\n",
      "|      16|        371|2024-03-18|Cancelled|      323.36|\n",
      "|      17|        138|2024-04-03|  Pending|      174.36|\n",
      "|      18|        816|2021-01-20|  Shipped|       75.39|\n",
      "|      19|        126|2021-03-09|Delivered|       41.17|\n",
      "|      20|        856|2021-06-08|  Shipped|      348.79|\n",
      "+--------+-----------+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "customers_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "\n",
    "# ✅ Generate Orders using only valid customer IDs\n",
    "def generate_orders(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Pending\", \"Shipped\", \"Delivered\", \"Cancelled\"]), \n",
    "          round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_id\", \"customer_id\", \"order_date\", \"status\", \"total_amount\"]\n",
    "    )\n",
    "\n",
    "orders_df = generate_orders(100000)\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fddd3d9-b125-46b3-91f7-084c438f86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "orders_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166d8144-9234-4f07-8872-1af7e64d46b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#orders_df.write.jdbc(url=MYSQL_URL, table=\"Orders\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "orders_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "valid_order_ids = [row[\"order_id\"] for row in orders_df.select(\"order_id\").collect()]\n",
    "valid_product_ids = [row[\"product_id\"] for row in products_df.select(\"product_id\").collect()]\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "# ✅ Generate Order Items using only valid order_id & product_id\n",
    "def generate_order_items(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_order_ids), random.choice(valid_product_ids), \n",
    "          random.randint(1, 5), round(random.uniform(5, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"order_item_id\", \"order_id\", \"product_id\", \"quantity\", \"unit_price\"]\n",
    "    )\n",
    "order_items_df = generate_order_items(100000)\n",
    "order_items_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Order_Items\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed681e4f-a7db-4b96-aed9-0d950de2e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate Payments\n",
    "def generate_payments(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_order_ids), fake.date_this_decade(), \n",
    "          random.choice([\"Credit Card\", \"PayPal\", \"Gift Card\", \"Bank Transfer\"]), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Success\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"payment_id\", \"order_id\", \"payment_date\", \"payment_method\", \"amount\", \"status\"]\n",
    "    )\n",
    "payments_df = generate_payments(10000)\n",
    "valid_payments_ids = [row[\"payment_id\"] for row in payments_df.select(\"payment_id\").collect()]\n",
    "\n",
    "# Generate Purchase History\n",
    "def generate_purchase_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          fake.date_this_decade(), round(random.uniform(10, 500), 2)) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"history_id\", \"customer_id\", \"order_id\", \"purchase_date\", \"amount_spent\"]\n",
    "    )\n",
    "\n",
    "payments_df.write.jdbc(url=ORDER_MYSQL_URL, table=\"Payments\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "purchase_history_df = generate_purchase_history(1000)\n",
    "purchase_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Purchase_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb8499c8-bf7a-4b00-bbf6-a38667e40133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to MySQL!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate Usage History\n",
    "def generate_usage_history(n):\n",
    "    return spark.createDataFrame(\n",
    "        [(i,random.choice(valid_customer_ids),random.choice(valid_product_ids),fake.date_this_decade(), fake.sentence(nb_words=10), \n",
    "          random.choice([\"Returned\", \"Not Returned\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"usage_id\", \"customer_id\", \"product_id\", \"usage_date\", \"feedback\", \"return_status\"]\n",
    "    )\n",
    "\n",
    "# Generate Transactions\n",
    "def generate_transactions(n,):\n",
    "    return spark.createDataFrame(\n",
    "        [(i, random.choice(valid_customer_ids), random.choice(valid_order_ids), \n",
    "          random.choice(valid_payments_ids), fake.date_this_decade(), \n",
    "          round(random.uniform(10, 500), 2), random.choice([\"Purchase\", \"Refund\"]), \n",
    "          random.choice([\"Completed\", \"Failed\", \"Pending\"])) \n",
    "         for i in range(1, n+1)], \n",
    "        [\"transaction_id\", \"customer_id\", \"order_id\", \"payment_id\", \"transaction_date\", \n",
    "         \"amount\", \"transaction_type\", \"status\"]\n",
    "    )\n",
    "usage_history_df = generate_usage_history(1000)\n",
    "usage_history_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Usage_History\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "transactions_df = generate_transactions(1000)\n",
    "transactions_df.distinct().write.jdbc(url=ORDER_MYSQL_URL, table=\"Transactions\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "print(\"✅ Data successfully written to MySQL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d069b0ab-89a1-4102-9c60-4c639a9b8dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+----------+-------------+\n",
      "|customer_id|     ip_address|login_attempt|login_date|is_suspicious|\n",
      "+-----------+---------------+-------------+----------+-------------+\n",
      "|          1|  192.168.39.36|      FAILURE| 2024-02-8|            1|\n",
      "|          1| 192.168.85.132|      FAILURE|2024-02-11|            0|\n",
      "|          1|    192.168.3.0|      FAILURE| 2024-02-6|            1|\n",
      "|          1|192.168.224.197|      FAILURE| 2024-02-2|            0|\n",
      "|          1|  192.168.54.36|      SUCCESS|2024-02-10|            0|\n",
      "|          1|192.168.123.248|      FAILURE|2024-02-24|            0|\n",
      "|          1|192.168.104.243|      SUCCESS|2024-02-14|            0|\n",
      "|          1| 192.168.126.81|      SUCCESS|2024-02-13|            0|\n",
      "|          1|192.168.118.229|      FAILURE|2024-02-28|            0|\n",
      "|          1|  192.168.95.87|      FAILURE|2024-02-23|            1|\n",
      "|          1| 192.168.254.32|      SUCCESS|2024-02-22|            0|\n",
      "|          1| 192.168.95.168|      FAILURE| 2024-02-2|            0|\n",
      "|          1|   192.168.5.81|      SUCCESS|2024-02-18|            0|\n",
      "|          1|192.168.127.249|      FAILURE|2024-02-24|            0|\n",
      "|          1| 192.168.26.181|      SUCCESS|2024-02-20|            0|\n",
      "|          1|192.168.138.190|      FAILURE|2024-02-28|            0|\n",
      "|          1|192.168.227.227|      FAILURE| 2024-02-8|            1|\n",
      "|          1| 192.168.35.164|      SUCCESS|2024-02-12|            0|\n",
      "|          1|192.168.158.210|      FAILURE| 2024-02-8|            0|\n",
      "|          1|192.168.140.129|      SUCCESS| 2024-02-2|            1|\n",
      "+-----------+---------------+-------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when,rand,lit\n",
    "# Generate random login data\n",
    "def generate_logins(customers_df, num_attempts=50):\n",
    "    logins = []\n",
    "    for row in customers_df.collect():\n",
    "        customer_id = row[\"customer_id\"]\n",
    "        num_logins = random.randint(1, num_attempts)  # Random login attempts per customer\n",
    "        \n",
    "        for _ in range(num_logins):\n",
    "            ip = f\"192.168.{random.randint(0, 255)}.{random.randint(0, 255)}\"\n",
    "            login_attempt = random.choice([\"SUCCESS\", \"FAILURE\"])\n",
    "            login_date = f\"2024-02-{random.randint(1, 28)}\"\n",
    "            \n",
    "            logins.append((customer_id, ip, login_attempt, login_date))\n",
    "    \n",
    "    return logins\n",
    "\n",
    "# Generate synthetic login data\n",
    "login_data = generate_logins(customers_df)\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"customer_id\", \"ip_address\", \"login_attempt\", \"login_date\"]\n",
    "login_df = spark.createDataFrame(login_data, columns)\n",
    "\n",
    "# Introduce suspicious activity (customers with >3 IPs or >10 attempts)\n",
    "login_df = login_df.withColumn(\"is_suspicious\", when((rand() < 0.2), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Write to MySQL (optional)\n",
    "login_df.write.jdbc(url=USER_MYSQL_URL, table=\"LoginHistory\", mode=\"append\", properties=MYSQL_PROPERTIES)\n",
    "\n",
    "# Show result\n",
    "login_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2696a-9227-43f4-939b-b782bf27dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
