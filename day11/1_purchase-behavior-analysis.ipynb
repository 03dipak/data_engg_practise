{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea14ef1-360c-41ec-b787-ebfac825fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68517c44-9c5d-44a9-b4b2-994dae247f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/glue_user/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/glue_user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/glue_user/.ivy2/jars\n",
      "mysql#mysql-connector-java added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d6a79007-d978-48df-8f06-2286d4379f3a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound mysql#mysql-connector-java;8.0.33 in central\n",
      "\tfound com.mysql#mysql-connector-j;8.0.33 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      ":: resolution report :: resolve 1081ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.mysql#mysql-connector-j;8.0.33 from central in [default]\n",
      "\tmysql#mysql-connector-java;8.0.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d6a79007-d978-48df-8f06-2286d4379f3a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/134ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/15 08:49:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/15 08:49:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session with MySQL JDBC Driver\n",
    "spark = SparkSession.builder.appName(\"RetailCustomer360\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ab4934-dc82-4ada-ae83-fc56430c1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MySQL table into PySpark DataFrame\n",
    "customer_df = spark.read.jdbc(url=USER_MYSQL_URL, table=\"Customers\", properties=MYSQL_PROPERTIES)\n",
    "order_df = spark.read.jdbc(url=ORDER_MYSQL_URL, table=\"Orders\", properties=MYSQL_PROPERTIES)\n",
    "products_df = spark.read.jdbc(url=PRODUCT_MYSQL_URL, table=\"Products\", properties=MYSQL_PROPERTIES)\n",
    "# Register as Temp Tables\n",
    "customer_df.createOrReplaceTempView(\"customers\")\n",
    "order_df.createOrReplaceTempView(\"orders\")\n",
    "products_df.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b4b686e-d898-4196-bfa1-2f328682c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+-------------------+\n",
      "|customer_id| first_name|last_name|               email|               phone|             address|               city|        state|zip_code|         country|         created_at|\n",
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+-------------------+\n",
      "|          1|    Michael|  Rodgers|katiecasey@exampl...|+1-664-301-3316x9...|USS Dudley\\nFPO A...|        North Traci|        Idaho|   38917|     Saint Lucia|2025-03-15 07:34:22|\n",
      "|          2|      Paula|    Berry|carolyn15@example...|  971-396-8026x98919|PSC 9301, Box 951...|       Hornechester|     Michigan|   14169|          Turkey|2025-03-15 07:34:32|\n",
      "|          3|     Angela| Lawrence|  kchung@example.org|   616-578-2644x4229|884 Brandi Pike A...|         New Kelsey|     Maryland|   38467|         Lesotho|2025-03-15 07:34:24|\n",
      "|          4|      Nancy|      May| carol17@example.org|   823-433-0320x8126|254 Watson Via Su...|       Andersonport|     Arkansas|   38645|         Vietnam|2025-03-15 07:34:37|\n",
      "|          5|Christopher|   Howell|melanie13@example...|+1-585-711-2536x7...|1818 William Gate...|       Spencerhaven|     New York|   64962|         Bahamas|2025-03-15 07:34:30|\n",
      "|          6|     Robert|  Sanchez|bishopmelissa@exa...|  (971)730-0494x4499|3114 White Port S...|           New Erin|         Iowa|   20071|         Mayotte|2025-03-15 07:34:22|\n",
      "|          7|     Joseph|      Kim|  owatts@example.org|     +1-834-620-5485|67994 Smith Light...|      Lake Paultown|     Missouri|   80184|           Samoa|2025-03-15 07:34:41|\n",
      "|          8|     Robert|   Holmes|speterson@example...|   (265)435-4021x622|025 Larson Cove S...|South Cassandrafurt|     Virginia|   67006|            Togo|2025-03-15 07:34:24|\n",
      "|          9|    Raymond| Robinson|   shart@example.com|  984.454.7585x90723|USNS Miller\\nFPO ...| North Melissahaven|     New York|   95746|         Nigeria|2025-03-15 07:34:37|\n",
      "|         10|    Michael|    Meyer|bethbeasley@examp...|        623.900.0817|67351 Jennifer Po...|   New Michaelmouth|    Wisconsin|   02747|     Timor-Leste|2025-03-15 07:34:34|\n",
      "|         11|       Erin|    Bruce|morrisonsteven@ex...|        386.671.8774|339 Frederick Mou...|      Lake Virginia|    Tennessee|   49628|            Guam|2025-03-15 07:34:36|\n",
      "|         12|     Joanna|Patterson|boyertrevor@examp...|   386.602.4277x8614|27315 David Track...|        Curtisburgh|      Indiana|   47235|    Saudi Arabia|2025-03-15 07:34:42|\n",
      "|         13|     Sandra|    Green|brandy26@example.com|   (964)384-1129x148|803 Danielle Plai...|         Sarahville|     Colorado|   21162|            Niue|2025-03-15 07:34:25|\n",
      "|         14|  Elizabeth|    White|paulasmith@exampl...|001-668-382-8172x...|23430 Joseph Lock...|          Kevinfurt|     Colorado|   78721|       Hong Kong|2025-03-15 07:34:31|\n",
      "|         15|    Rebecca|  Robbins|imontgomery@examp...|    566.929.3785x722|345 Ashley Cove\\n...|        East Thomas| South Dakota|   02378|      Mozambique|2025-03-15 07:34:41|\n",
      "|         16|    Melissa|   Jordan|dzimmerman@exampl...|    391-346-4047x786|9879 Smith Juncti...|  Lake Davidchester|       Alaska|   80105|          Jordan|2025-03-15 07:34:36|\n",
      "|         17|    Timothy|  Griffin|  yfrank@example.net|        542-579-3866|62163 Jodi View S...|        Jacksonberg|West Virginia|   17423|            Mali|2025-03-15 07:34:25|\n",
      "|         18|    Patrick|    Davis|rmartinez@example...|  (414)470-1021x7542|94543 Gutierrez T...| North Briannamouth|      Vermont|   22272|Marshall Islands|2025-03-15 07:34:42|\n",
      "|         19|       Kara|   Harvey|butleradrian@exam...| (665)460-5980x89623|63257 Larry Trail...|       Russellshire|     Oklahoma|   13607|      San Marino|2025-03-15 07:34:39|\n",
      "|         20|      Scott|     King| holly99@example.org|   302.957.4187x3470|2212 Dawson Club\\...|        Riveramouth|   New Jersey|   06081|    South Africa|2025-03-15 07:34:36|\n",
      "+-----------+-----------+---------+--------------------+--------------------+--------------------+-------------------+-------------+--------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133c907a-d105-4925-a326-84ea17dee80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+---------+------------+\n",
      "|order_id|customer_id|         order_date|   status|total_amount|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "|     386|        553|2025-01-18 00:00:00|  Pending|      141.46|\n",
      "|    1079|        553|2022-07-29 00:00:00|Delivered|      384.93|\n",
      "|    2262|        553|2024-05-03 00:00:00|Cancelled|      165.35|\n",
      "|    3996|        553|2021-02-05 00:00:00|  Shipped|       69.20|\n",
      "|    5192|        553|2021-07-25 00:00:00|  Pending|      378.97|\n",
      "|    5646|        553|2023-07-04 00:00:00|  Shipped|      276.54|\n",
      "|    6977|        553|2023-07-30 00:00:00|  Pending|      351.98|\n",
      "|    8405|        553|2022-04-12 00:00:00|  Pending|      466.65|\n",
      "|    9451|        553|2022-01-01 00:00:00|Delivered|      446.68|\n",
      "|    9688|        553|2023-04-30 00:00:00|  Shipped|      345.45|\n",
      "|   10400|        553|2023-05-16 00:00:00|  Shipped|      315.09|\n",
      "|   11051|        553|2020-02-23 00:00:00|Cancelled|      339.56|\n",
      "|   11081|        553|2022-03-13 00:00:00|  Pending|      361.35|\n",
      "|   11133|        553|2023-05-14 00:00:00|Delivered|      401.09|\n",
      "|   12560|        553|2025-02-28 00:00:00|  Shipped|      104.33|\n",
      "|   13386|        553|2022-09-20 00:00:00|Cancelled|      346.99|\n",
      "|   13396|        553|2023-01-24 00:00:00|  Shipped|       85.40|\n",
      "|   13455|        553|2020-12-25 00:00:00|Cancelled|       96.82|\n",
      "|   14381|        553|2021-04-07 00:00:00|  Shipped|      312.33|\n",
      "|   14901|        553|2024-07-03 00:00:00|  Pending|      142.08|\n",
      "|   15140|        553|2024-11-17 00:00:00|Delivered|      340.27|\n",
      "|   15239|        553|2021-06-05 00:00:00|Delivered|      234.89|\n",
      "|   16013|        553|2021-04-05 00:00:00|Cancelled|      452.03|\n",
      "|   16065|        553|2024-02-24 00:00:00|  Shipped|      300.94|\n",
      "|   17036|        553|2023-08-11 00:00:00|  Shipped|      118.74|\n",
      "|   17286|        553|2023-12-19 00:00:00|Cancelled|       79.79|\n",
      "|   18601|        553|2024-04-17 00:00:00|Delivered|      236.63|\n",
      "|   19159|        553|2025-03-04 00:00:00|  Pending|      217.91|\n",
      "|   19916|        553|2021-08-17 00:00:00|  Pending|      205.41|\n",
      "|   20311|        553|2024-05-06 00:00:00|  Shipped|      320.56|\n",
      "|   20787|        553|2024-01-11 00:00:00|Delivered|      318.89|\n",
      "|   20968|        553|2023-05-29 00:00:00|  Shipped|      402.99|\n",
      "|   24385|        553|2020-06-25 00:00:00|Cancelled|      432.71|\n",
      "|   25133|        553|2020-05-02 00:00:00|Cancelled|      303.03|\n",
      "|   25699|        553|2024-12-23 00:00:00|  Pending|      271.07|\n",
      "|   27392|        553|2020-11-26 00:00:00|  Pending|      146.33|\n",
      "|   27496|        553|2020-12-08 00:00:00|  Pending|       41.33|\n",
      "|   28096|        553|2021-09-13 00:00:00|Delivered|      213.37|\n",
      "|   28145|        553|2020-03-04 00:00:00|Cancelled|       17.76|\n",
      "|   28213|        553|2022-01-02 00:00:00|Delivered|      430.81|\n",
      "|   29802|        553|2022-02-24 00:00:00|  Pending|      474.92|\n",
      "|   29948|        553|2022-01-22 00:00:00|Delivered|       67.97|\n",
      "|   30425|        553|2024-02-15 00:00:00|Cancelled|      289.47|\n",
      "|   30730|        553|2022-02-18 00:00:00|Cancelled|      415.23|\n",
      "|   31012|        553|2023-11-06 00:00:00|Delivered|       14.71|\n",
      "|   31383|        553|2021-07-22 00:00:00|Delivered|      368.72|\n",
      "|   32825|        553|2023-01-04 00:00:00|  Pending|      422.92|\n",
      "|   33481|        553|2023-06-24 00:00:00|  Pending|      374.83|\n",
      "|   33685|        553|2021-01-27 00:00:00|Cancelled|      171.16|\n",
      "|   34699|        553|2025-02-07 00:00:00|  Shipped|      469.20|\n",
      "|   35281|        553|2024-03-15 00:00:00|Delivered|      378.29|\n",
      "|   36265|        553|2023-07-04 00:00:00|  Shipped|       34.55|\n",
      "|   37713|        553|2021-06-17 00:00:00|  Pending|      376.28|\n",
      "|   38062|        553|2021-04-13 00:00:00|Delivered|      244.65|\n",
      "|   38070|        553|2022-05-07 00:00:00|  Shipped|      138.46|\n",
      "|   41867|        553|2020-07-22 00:00:00|Cancelled|       24.78|\n",
      "|   43682|        553|2022-10-23 00:00:00|Cancelled|      241.25|\n",
      "|   44477|        553|2020-11-02 00:00:00|Cancelled|      153.59|\n",
      "|   45032|        553|2022-09-19 00:00:00|Delivered|      371.65|\n",
      "|   45059|        553|2024-10-04 00:00:00|  Shipped|      400.04|\n",
      "|   46007|        553|2024-09-30 00:00:00|Cancelled|       18.21|\n",
      "|   48930|        553|2022-05-07 00:00:00|Delivered|      122.89|\n",
      "|   49985|        553|2020-05-01 00:00:00|Cancelled|      231.29|\n",
      "|   50462|        553|2024-07-23 00:00:00|  Shipped|      292.44|\n",
      "|   51080|        553|2023-10-28 00:00:00|Cancelled|      285.84|\n",
      "|   51678|        553|2021-05-24 00:00:00|  Pending|      423.56|\n",
      "|   54339|        553|2021-08-09 00:00:00|  Shipped|      296.32|\n",
      "|   54494|        553|2023-04-03 00:00:00|Delivered|       60.49|\n",
      "|   54539|        553|2022-07-16 00:00:00|Delivered|      212.40|\n",
      "|   57352|        553|2022-01-19 00:00:00|Cancelled|      116.05|\n",
      "|   57885|        553|2024-05-26 00:00:00|Delivered|      203.31|\n",
      "|   59319|        553|2020-09-30 00:00:00|  Shipped|      106.28|\n",
      "|   60888|        553|2021-11-20 00:00:00|  Shipped|      255.24|\n",
      "|   60918|        553|2021-03-31 00:00:00|Delivered|      485.40|\n",
      "|   61011|        553|2024-03-10 00:00:00|  Pending|      361.32|\n",
      "|   69187|        553|2021-05-01 00:00:00|  Pending|       78.13|\n",
      "|   69786|        553|2020-04-04 00:00:00|Cancelled|      481.65|\n",
      "|   73168|        553|2021-05-07 00:00:00|  Shipped|       43.05|\n",
      "|   73919|        553|2022-01-17 00:00:00|  Shipped|       48.89|\n",
      "|   75251|        553|2024-01-06 00:00:00|Delivered|       88.71|\n",
      "|   76208|        553|2020-09-16 00:00:00|  Pending|       98.52|\n",
      "|   76224|        553|2023-09-19 00:00:00|  Pending|       42.93|\n",
      "|   77560|        553|2024-01-06 00:00:00|Cancelled|      377.81|\n",
      "|   77807|        553|2024-01-14 00:00:00|  Shipped|      311.89|\n",
      "|   79493|        553|2024-12-24 00:00:00|  Pending|      461.80|\n",
      "|   79517|        553|2024-01-09 00:00:00|  Pending|      112.14|\n",
      "|   80166|        553|2022-02-08 00:00:00|  Shipped|      115.22|\n",
      "|   81998|        553|2022-04-01 00:00:00|  Shipped|      269.40|\n",
      "|   82265|        553|2021-11-12 00:00:00|Delivered|       67.30|\n",
      "|   83838|        553|2022-04-14 00:00:00|  Pending|       14.81|\n",
      "|   85335|        553|2020-05-27 00:00:00|Delivered|       16.74|\n",
      "|   86158|        553|2023-08-06 00:00:00|Delivered|      250.29|\n",
      "|   87340|        553|2022-08-16 00:00:00|  Shipped|      160.10|\n",
      "|   88214|        553|2023-07-16 00:00:00|Delivered|       94.91|\n",
      "|   88353|        553|2020-11-23 00:00:00|  Shipped|      198.75|\n",
      "|   89386|        553|2023-12-19 00:00:00|  Pending|      491.38|\n",
      "|   89548|        553|2020-10-03 00:00:00|  Pending|      470.58|\n",
      "|   90119|        553|2021-08-08 00:00:00|  Shipped|      161.08|\n",
      "|   90680|        553|2023-03-12 00:00:00|Cancelled|      349.30|\n",
      "|   91683|        553|2020-12-21 00:00:00|Delivered|       22.39|\n",
      "+--------+-----------+-------------------+---------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *   FROM orders o  where o.customer_id=553 \"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b1c3429-acc6-4d94-adc9-7cae792f5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|        553|    6881.39|\n",
      "|       6188|    6646.06|\n",
      "|       6940|    6545.92|\n",
      "|       5361|    6519.63|\n",
      "|       7440|    6292.82|\n",
      "|       8459|    6152.89|\n",
      "|       1023|    6071.49|\n",
      "|       6208|    6063.56|\n",
      "|       4607|    6030.39|\n",
      "|       7658|    5993.29|\n",
      "|         70|    5957.60|\n",
      "|       6305|    5912.17|\n",
      "|       7153|    5897.98|\n",
      "|       5231|    5853.98|\n",
      "|       1821|    5799.72|\n",
      "|       2667|    5797.99|\n",
      "|       1524|    5790.05|\n",
      "|       2242|    5766.12|\n",
      "|       6724|    5764.37|\n",
      "|       3469|    5670.20|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent\n",
    "        FROM orders o GROUP BY o.customer_id  order by total_spent desc\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190428bf-3f49-49c1-a603-baf117748d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+-------------------+\n",
      "|customer_id|total_spent|total_orders| last_purchase_date|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "|        553|    6881.39|          20|2025-01-22 00:00:00|\n",
      "|       6188|    6646.06|          20|2024-09-27 00:00:00|\n",
      "|       6940|    6545.92|          20|2025-02-23 00:00:00|\n",
      "|       5361|    6519.63|          20|2024-11-18 00:00:00|\n",
      "|       7440|    6292.82|          21|2024-09-24 00:00:00|\n",
      "|       8459|    6152.89|          22|2024-12-26 00:00:00|\n",
      "|       1023|    6071.49|          20|2024-09-11 00:00:00|\n",
      "|       6208|    6063.56|          20|2024-11-09 00:00:00|\n",
      "|       4607|    6030.39|          19|2024-12-01 00:00:00|\n",
      "|       7658|    5993.29|          20|2025-01-21 00:00:00|\n",
      "+-----------+-----------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_customers = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o \n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc  \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570d5292-2fb0-4944-8b31-b8949fec3638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|       country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Kingdom|         70| Stephanie|donnajones@exampl...|    1251.89|           4|2024-09-28 00:00:00|            1|\n",
      "|United Kingdom|       2185|   Tiffany|johnsonjennifer@e...|    1234.75|           4|2025-02-18 00:00:00|            2|\n",
      "|United Kingdom|       7597|     Tanya|  iwoods@example.org|    1221.33|           3|2024-12-18 00:00:00|            3|\n",
      "|United Kingdom|       4895|  Jennifer|emilymanning@exam...|     917.19|           3|2024-12-14 00:00:00|            4|\n",
      "|United Kingdom|       4264| Christian|tfleming@example.net|     843.86|           3|2024-09-21 00:00:00|            5|\n",
      "|United Kingdom|        402|     Kevin|christinaadkins@e...|     820.48|           4|2025-03-10 00:00:00|            6|\n",
      "|United Kingdom|       3666|   Anthony|justin26@example.net|     806.65|           3|2025-02-05 00:00:00|            7|\n",
      "|United Kingdom|       2731|     Bruce|johnmoore@example...|     802.29|           4|2025-03-07 00:00:00|            8|\n",
      "|United Kingdom|       9974|    Robert|amanda58@example.net|     745.25|           2|2024-11-12 00:00:00|            9|\n",
      "|United Kingdom|       4856|     Wendy|jeffrey97@example...|     655.46|           2|2024-03-16 00:00:00|           10|\n",
      "|United Kingdom|       3254|    Daniel| ldaniel@example.org|     612.16|           3|2025-02-18 00:00:00|           11|\n",
      "|United Kingdom|       7385|    Edward|   ypope@example.com|     576.72|           2|2025-01-24 00:00:00|           12|\n",
      "|United Kingdom|       1450|     James|blakechristopher@...|     401.92|           1|2024-08-08 00:00:00|           13|\n",
      "|United Kingdom|       9837|  Jennifer|stephanie67@examp...|     377.55|           3|2025-01-01 00:00:00|           14|\n",
      "|United Kingdom|       6934|     Jason|josephgilbert@exa...|     333.04|           2|2024-12-08 00:00:00|           15|\n",
      "|United Kingdom|       9692|      Sara|briansanchez@exam...|     284.16|           1|2024-10-02 00:00:00|           16|\n",
      "|United Kingdom|       1671|     James|ssanchez@example.org|     258.46|           1|2025-01-08 00:00:00|           17|\n",
      "|United Kingdom|       9536|      Lisa| frank03@example.net|     233.59|           2|2024-04-12 00:00:00|           18|\n",
      "|United Kingdom|        934|    Hayden|markrivera@exampl...|     211.86|           1|2025-01-13 00:00:00|           19|\n",
      "|United Kingdom|       7252|     Kevin|  lisa07@example.com|     152.86|           1|2024-06-07 00:00:00|           20|\n",
      "|United Kingdom|       2189|   Tiffany|farrelldanny@exam...|     138.13|           1|2024-10-13 00:00:00|           21|\n",
      "|United Kingdom|       4930|    Cheryl|campbellsean@exam...|      39.63|           1|2025-02-01 00:00:00|           22|\n",
      "+--------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)   -- Last 1 year\n",
    "        GROUP BY o.customer_id order by total_spent desc \n",
    "    )\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER ( PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id where c.country='United Kingdom'  GROUP BY c.country,c.customer_id,c.first_name,c.email,cs.total_spent,cs.last_purchase_date,cs.total_orders order by total_spent desc\n",
    "\"\"\").show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "958dee8b-983b-4384-86e0-befda5cbddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|             country|customer_id|first_name|               email|total_spent|total_orders| last_purchase_date|spending_rank|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "|United Arab Emirates|       5354|      Ryan|lisamorrison@exam...|    1565.06|           5|2025-02-27 00:00:00|            1|\n",
      "|United Arab Emirates|       6218| Katherine|walkerrobert@exam...|    1473.88|           4|2024-10-15 00:00:00|            2|\n",
      "|United Arab Emirates|       8647|  Danielle|  bsmith@example.com|    1367.50|           5|2025-02-11 00:00:00|            3|\n",
      "|United Arab Emirates|        110|  Samantha|stephenobrien@exa...|    1218.35|           4|2024-11-09 00:00:00|            4|\n",
      "|United Arab Emirates|       5727|     Kevin| david25@example.com|    1153.08|           4|2025-01-25 00:00:00|            5|\n",
      "|United Arab Emirates|       6464|     Sarah|traceyyoung@examp...|    1126.01|           4|2024-11-28 00:00:00|            6|\n",
      "|United Arab Emirates|       9201| Christina|   xcarr@example.com|    1064.73|           4|2025-03-01 00:00:00|            7|\n",
      "|United Arab Emirates|        750|    Joanne|mhernandez@exampl...|    1042.31|           3|2024-12-24 00:00:00|            8|\n",
      "|United Arab Emirates|       2524|  Samantha|mackmiguel@exampl...|     909.08|           3|2025-03-12 00:00:00|            9|\n",
      "|United Arab Emirates|       6765|     Robin|  ohenry@example.com|     860.18|           3|2024-06-29 00:00:00|           10|\n",
      "|      United Kingdom|         70| Stephanie|donnajones@exampl...|    1251.89|           4|2024-09-28 00:00:00|            1|\n",
      "|      United Kingdom|       2185|   Tiffany|johnsonjennifer@e...|    1234.75|           4|2025-02-18 00:00:00|            2|\n",
      "|      United Kingdom|       7597|     Tanya|  iwoods@example.org|    1221.33|           3|2024-12-18 00:00:00|            3|\n",
      "|      United Kingdom|       4895|  Jennifer|emilymanning@exam...|     917.19|           3|2024-12-14 00:00:00|            4|\n",
      "|      United Kingdom|       4264| Christian|tfleming@example.net|     843.86|           3|2024-09-21 00:00:00|            5|\n",
      "|      United Kingdom|        402|     Kevin|christinaadkins@e...|     820.48|           4|2025-03-10 00:00:00|            6|\n",
      "|      United Kingdom|       3666|   Anthony|justin26@example.net|     806.65|           3|2025-02-05 00:00:00|            7|\n",
      "|      United Kingdom|       2731|     Bruce|johnmoore@example...|     802.29|           4|2025-03-07 00:00:00|            8|\n",
      "|      United Kingdom|       9974|    Robert|amanda58@example.net|     745.25|           2|2024-11-12 00:00:00|            9|\n",
      "|      United Kingdom|       4856|     Wendy|jeffrey97@example...|     655.46|           2|2024-03-16 00:00:00|           10|\n",
      "|United States Min...|       9751|    Robert|miranda58@example...|    1485.21|           4|2025-01-01 00:00:00|            1|\n",
      "|United States Min...|       8089|     Donna|diamond79@example...|    1343.53|           4|2025-02-22 00:00:00|            2|\n",
      "|United States Min...|       2918|     Peter|christine02@examp...|    1305.35|           3|2025-03-06 00:00:00|            3|\n",
      "|United States Min...|       9004|     Erika|paulprince@exampl...|    1093.32|           3|2025-02-25 00:00:00|            4|\n",
      "|United States Min...|       6977|     Jimmy|daniel38@example.com|     859.77|           2|2024-09-03 00:00:00|            5|\n",
      "|United States Min...|       1290|    Ashley|leslie12@example.org|     800.48|           2|2025-02-24 00:00:00|            6|\n",
      "|United States Min...|       5883|   Colleen|   tbond@example.net|     766.58|           2|2025-02-10 00:00:00|            7|\n",
      "|United States Min...|       1288|     Derek|theresa48@example...|     753.53|           3|2025-03-02 00:00:00|            8|\n",
      "|United States Min...|       3997|     Sarah| linda06@example.com|     743.42|           2|2025-01-06 00:00:00|            9|\n",
      "|United States Min...|       3512|      Gina|lorettasmith@exam...|     672.64|           4|2025-01-10 00:00:00|           10|\n",
      "|United States Vir...|        634|      Erik|alexmayo@example.org|    1880.66|           6|2025-02-17 00:00:00|            1|\n",
      "|United States Vir...|       3790|    Edward|christinabarnes@e...|    1701.86|           7|2025-02-12 00:00:00|            2|\n",
      "|United States Vir...|       8472|    Andrew| uingram@example.com|    1271.24|           4|2025-03-02 00:00:00|            3|\n",
      "|United States Vir...|        736|   Jeffrey|penadanielle@exam...|    1109.54|           4|2025-01-07 00:00:00|            4|\n",
      "|United States Vir...|       1401|  Savannah|quinnmarie@exampl...|     938.41|           3|2024-12-29 00:00:00|            5|\n",
      "|United States Vir...|       9923|     Mandy| gnewman@example.com|     781.31|           2|2025-02-06 00:00:00|            6|\n",
      "|United States Vir...|        643|    Dennis|tracywright@examp...|     766.40|           3|2025-03-02 00:00:00|            7|\n",
      "|United States Vir...|        243|   Kaitlyn|jamesmoore@exampl...|     753.64|           3|2025-01-17 00:00:00|            8|\n",
      "|United States Vir...|       3069| Stephanie|jenniferrowe@exam...|     742.46|           4|2025-01-15 00:00:00|            9|\n",
      "|United States Vir...|       7666|    Robert|kennethmills@exam...|     601.33|           2|2025-01-06 00:00:00|           10|\n",
      "|United States of ...|       5891|   Michael|alexander79@examp...|    1627.40|           4|2025-02-14 00:00:00|            1|\n",
      "|United States of ...|       8157|     Kevin|garciamichael@exa...|    1237.42|           4|2025-02-28 00:00:00|            2|\n",
      "|United States of ...|        295|   Rebecca|tgonzalez@example...|    1177.23|           3|2024-07-14 00:00:00|            3|\n",
      "|United States of ...|        770|  Jennifer| ubailey@example.net|    1091.77|           3|2024-12-20 00:00:00|            4|\n",
      "|United States of ...|       5315|    Kelsey|brookeknox@exampl...|    1090.99|           3|2025-03-06 00:00:00|            5|\n",
      "|United States of ...|        950|  Nicholas|gilbertzachary@ex...|    1001.88|           5|2025-02-13 00:00:00|            6|\n",
      "|United States of ...|       1600|   Natalie| ogibson@example.com|     927.53|           3|2024-10-09 00:00:00|            7|\n",
      "|United States of ...|       8129|      Mark|jensenstephanie@e...|     917.18|           2|2025-01-03 00:00:00|            8|\n",
      "|United States of ...|       6415|    Thomas| rhester@example.com|     722.75|           3|2024-11-13 00:00:00|            9|\n",
      "|United States of ...|       6925|    Dustin|michelletorres@ex...|     700.75|           2|2024-06-15 00:00:00|           10|\n",
      "+--------------------+-----------+----------+--------------------+-----------+------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Spark SQL Query\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT\n",
    "            o.customer_id,\n",
    "            SUM(o.total_amount) AS total_spent,\n",
    "            COUNT(o.order_id) AS total_orders,\n",
    "            MAX(o.order_date) AS last_purchase_date\n",
    "        FROM orders o\n",
    "        WHERE o.order_date >= date_add(current_date(), -365)  -- Last 1 year\n",
    "        GROUP BY o.customer_id\n",
    "    ),\n",
    "    customer_ranking AS (\n",
    "        SELECT\n",
    "            c.country,\n",
    "            c.customer_id,\n",
    "            c.first_name,\n",
    "            c.email,\n",
    "            cs.total_spent,\n",
    "            cs.total_orders,\n",
    "            cs.last_purchase_date,\n",
    "            RANK() OVER (PARTITION BY c.country ORDER BY cs.total_spent DESC) AS spending_rank\n",
    "        FROM customer_spending cs\n",
    "        JOIN customers c ON cs.customer_id = c.customer_id\n",
    "    )\n",
    "    SELECT * FROM customer_ranking WHERE spending_rank <= 10 and country like 'United %';\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "top_customers.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b9c2e-8f02-4d45-beb5-94528c3ecbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Filter last 1 year of data\n",
    "one_year_ago = F.date_add(F.current_date(), -365)\n",
    "filtered_orders = order_df.filter(F.col(\"order_date\") >= one_year_ago)\n",
    "\n",
    "# Aggregate customer spending\n",
    "customer_spending = (\n",
    "    filtered_orders.groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.max(\"order_date\").alias(\"last_purchase_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join with customers table\n",
    "customer_data = customer_spending.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Define window specification for ranking\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.desc(\"total_spent\"))\n",
    "\n",
    "# Add ranking column\n",
    "customer_ranking = customer_data.withColumn(\"spending_rank\", F.rank().over(window_spec))\n",
    "\n",
    "# Filter top 100 customers\n",
    "top_customers = customer_ranking.filter( (F.col(\"spending_rank\") <= 10) &  (F.col(\"country\").like(\"United %\")))\n",
    "\n",
    "# Show results\n",
    "top_customers.select(\"country\",\"customer_id\",\"first_name\",\"email\",\"total_spent\",\"total_orders\",\"spending_rank\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3abae0a8-6c0d-4787-87bc-bffa5522979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 04:38:47 INFO HiveConf: Found configuration file file:/home/glue_user/spark/conf/hive-site.xml\n",
      "25/03/13 04:38:48 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/13 04:38:48 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1605)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1594)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:615)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:248)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:248)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:151)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:141)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:642)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 49 more\n",
      "25/03/13 04:38:48 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/13 04:38:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/13 04:39:02 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=8aef4fb0-cd4c-4909-9690-b322e086b826, clientType=HIVECLI]\n",
      "25/03/13 04:39:02 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/03/13 04:39:02 INFO AWSCatalogMetastoreClient: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "25/03/13 04:39:02 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/13 04:39:02 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:917)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:881)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1483)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1154)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:180)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:122)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1104)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 83 more\n",
      "25/03/13 04:39:02 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n",
      "25/03/13 04:39:02 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/13 04:39:02 WARN EC2MetadataUtils: Unable to retrieve the requested metadata (/latest/dynamic/instance-identity/document). Failed to connect to service endpoint: \n",
      "com.amazonaws.SdkClientException: Failed to connect to service endpoint: \n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)\n",
      "\tat com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getItems(EC2MetadataUtils.java:407)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:376)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getData(EC2MetadataUtils.java:372)\n",
      "\tat com.amazonaws.util.EC2MetadataUtils.getEC2InstanceRegion(EC2MetadataUtils.java:287)\n",
      "\tat com.amazonaws.regions.Regions.getCurrentRegion(Regions.java:109)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueClientFactory.newClient(AWSGlueClientFactory.java:64)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSCatalogMetastoreClient.<init>(AWSCatalogMetastoreClient.java:142)\n",
      "\tat com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory.createMetaStoreClient(AWSGlueDataCatalogHiveClientFactory.java:20)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.createMetaStoreClient(HiveUtils.java:507)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3856)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3836)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:877)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:892)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:620)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:554)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:552)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:540)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:429)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:298)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:104)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:269)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:373)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:701)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:679)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:573)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:465)\n",
      "\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:560)\n",
      "\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:244)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:341)\n",
      "\tat sun.net.www.http.HttpClient.New(HttpClient.java:359)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056)\n",
      "\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990)\n",
      "\tat com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)\n",
      "\tat com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)\n",
      "\t... 79 more\n",
      "25/03/13 04:39:02 INFO AWSGlueClientFactory: No region info found, using SDK default region: us-east-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sales data written to S3: s3://feb2025-training-bucket/analytics/top_100_customers/\n",
      "Glue table 'customer_analytics.top_100_customers' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define AWS Glue database and table names\n",
    "glue_database = \"customer_analytics\"\n",
    "glue_table = \"top_100_customers\"\n",
    "\n",
    "# Define S3 output path\n",
    "s3_output_path = \"s3://feb2025-training-bucket/analytics/top_100_customers/\"\n",
    "\n",
    "# Create the AWS Glue Catalog table using the DataFrame\n",
    "top_customers.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", s3_output_path) \\\n",
    "    .saveAsTable(f\"{glue_database}.{glue_table}\")\n",
    "\n",
    "print(f\"Aggregated sales data written to S3: {s3_output_path}\")\n",
    "print(f\"Glue table '{glue_database}.{glue_table}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd734bb2-b506-49cc-a469-f69517a718a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
